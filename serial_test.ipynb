{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Classification Project\n",
    "\n",
    "*a thinkful project by Kalika Kay Curry*\n",
    "\n",
    "Build a project to classify text-author.\n",
    "\n",
    "Collect thousand texts from Gutenberg project (and 7 novels) for at least 10 authors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "import pandas as pd\n",
    "import en_core_web_sm\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation that spaCy doesn't\n",
    "    # recognize: the double dash --. Better get rid of it now!\n",
    "    text = re.sub(r\"\\r?\\n|\\r\", \" \", text)\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_frequencies(text):\n",
    "    length = [len(w) for w in text]\n",
    "    fdist = nltk.FreqDist(length)\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to calculate how frequently words appear in the text\n",
    "def word_frequencies(text):\n",
    "    \n",
    "    # Build a list of words\n",
    "    # Strip out punctuation\n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct:\n",
    "            words.append(token.text)\n",
    "            \n",
    "    # Build and return a `Counter` object containing word counts\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(text):\n",
    "    print(f'{dt.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listtostrings(s):\n",
    "    return ' '.join([str(elem) for elem in s]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/12/2020 20:26:57, started\n",
      "26/12/2020 20:49:35, ended\n"
     ]
    }
   ],
   "source": [
    "#Thousands of Gutenberg texts.\n",
    "print(f'{dt.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}, started')\n",
    "text2000 = []\n",
    "i = 9\n",
    "count = 0\n",
    "while count != 2000:    \n",
    "    try:\n",
    "        url = f'http://www.gutenberg.org/files/{i}/{i}.txt'\n",
    "        response = request.urlopen(url)\n",
    "        raw = response.read().decode('utf8')\n",
    "        text2000.append(raw )\n",
    "        count += 1\n",
    "        i += 1\n",
    "    except:\n",
    "        i += 1\n",
    "print(f'{dt.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}, ended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authors\n",
    "\n",
    "The texts from the Gutenberg library does not provide authors in a seperate field. Even the html files don't contain a seperate author field for identifying, easily, who the work is by; so I can't really run a classifier on this yet. \n",
    "\n",
    "At the same time, I'm curious who these 2,000 works are by - how many of each author do I have? So a little bit of data exploration and preprocessing to get the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From working with gutenberg data in previous examples, I know that most authors appear within the first 150 characters of the text. Usually it's less than 150 characters, somewhere in the 75 character range. I want the first 150 characters because of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The Project Gutenberg EBook of Roget's Thesaurus, by Peter Mark Roget This eBook is for the use of anyone anywhere at no cost and with almost no r\", 'The Project Gutenberg EBook of The Narrative of the Life of Frederick Douglass, by Frederick Douglass This eBook is for the use of anyone anywhe', 'The Project Gutenberg EBook of O Pioneers!, by Willa Cather This eBook is for the use of anyone anywhere at no cost and with almost no restriction', 'The Project Gutenberg EBook of The CIA World Factbook, by United States. Central Intelligence Agency. This eBook is for the use of anyone a', 'The Project Gutenberg EBook of Paradise Lost, by John Milton ******************************************************************* THIS EBOOK WAS']\n"
     ]
    }
   ],
   "source": [
    "#Authors names usually appear within the first 150 characters. \n",
    "authors = []\n",
    "for text in text2000:\n",
    "    authors.append(text_cleaner(text[0:150]))\n",
    "print(authors[10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abraham', 'Charles', 'Lewis', 'United', 'James', 'Anonymous', 'Alexander', 'Henry', 'John', 'Peter', 'Frederick', 'Willa', 'United', 'John', 'Thomas']\n"
     ]
    }
   ],
   "source": [
    "#Authors will appears after the word \"by\"\n",
    "tokens = nltk.word_tokenize(listtostrings(authors))\n",
    "authortext = nltk.Text(tokens)\n",
    "\n",
    "#abandoing this method for - reasons.\n",
    "first = nltk.text.ConcordanceIndex(authortext).offsets('by') \n",
    "example = []\n",
    "for x in first:\n",
    "    example.append(authortext[x+1])\n",
    "print(example[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Looking at how often the words Gutenberg and by appear in the text.\n",
    "authortext.dispersion_plot(['by', 'Gutenberg', 'Anonymous', 'Various', 'This', 'Author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 2007 matches:\n",
      "The Project Gutenberg EBook of Lincoln 's First Inaugura\n",
      " of anyone anywhere a ﻿The Project Gutenberg EBook of The King James Bible * * \n",
      " * EBOOK ( # ) WAS ONE The Project Gutenberg EBook of Through the Looking-Glass\n",
      "eBook is for the use o The Project Gutenberg EBook of The Hunting of the Snark \n",
      "o cost and with almost The Project Gutenberg EBook of The CIA World Factbook , \n",
      "r the use of anyone an The Project Gutenberg EBook of Peter Pan , by James M. B\n",
      "h almost no restrictio The Project Gutenberg EBook of The Book Of Mormon , by A\n",
      "with almost no restric The Project Gutenberg EBook of The Federalist Papers , b\n",
      "r the use of anyone an The Project Gutenberg EBook of The Song Of Hiawatha , by\n",
      "no cost and with almos The Project Gutenberg EBook of Paradise Lost , by John M\n",
      "h almost no restrictio The Project Gutenberg EBook of Roget 's Thesaurus , by P\n",
      "t and with almost no r The Project Gutenberg EBook of The Narrative of the Life\n",
      "e use of anyone anywhe The Project Gutenberg EBook of O Pioneers ! , by Willa C\n",
      " almost no restriction The Project Gutenberg EBook of The CIA World Factbook , \n",
      "or the use of anyone a The Project Gutenberg EBook of Paradise Lost , by John M\n",
      "* * * * THIS EBOOK WAS The Project Gutenberg EBook of Far from the Madding Crow\n",
      " * * * * * * * * * THI The Project Gutenberg EBook of Aesop 's Fables , by Aeso\n",
      "ost no restrictions wh The Project Gutenberg EBook of Census of Population and \n",
      "k is for the use of anyone Project Gutenberg 's The Bible , King James Version \n",
      "ost and with almost no The Project Gutenberg EBook of The Oedipus Trilogy , by \n",
      " with almost no restri The Project Gutenberg EBook of Herland , by Charlotte Pe\n",
      "no cost and with almos The Project Gutenberg EBook of The Scarlet Letter , by N\n",
      "at no cost and with almost Project Gutenberg 's Zen and the Art of the Internet\n",
      "cost and with almost n The Project Gutenberg EBook of The War of the Worlds , b\n",
      " and with almost no re The Project Gutenberg EBook of Census of Population and \n"
     ]
    }
   ],
   "source": [
    "authortext.concordance('Gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abraham Lincoln', 'he Project Gutenberg EBook of The King James Bible', 'Charles Dodgson AKA Lewis Carroll', 'Lewis Carroll', 'United States. Central Intelligence Agency', 'James M. Barrie', 'Anonymous', 'Alexander Hamilton and John Jay and James Madison', 'Henry W. Longfellow', 'John Milton', 'Peter Mark Roget', 'Frederick Douglass', 'Willa Cather', 'United States. Central Intelligence Agency.', 'John Milton', 'Thomas Hardy', 'Aesop', 'United States Bureau of the Census', 'Various', 'Sophocles', 'Charlotte Perkins Stetson Gilman', 'Nathaniel Hawthorne', 'Brendan P. Kehoe', 'H. G. Wells', 'United States Bureau of the Census']\n"
     ]
    }
   ],
   "source": [
    "#This is the best that I can do. \n",
    "name = []\n",
    "for x in authors:\n",
    "    first = x.find('by') + len('by ')\n",
    "    last = x.find('This') \n",
    "    if last == -1:\n",
    "        last = x.find('*')\n",
    "    name.append(text_cleaner(x[first:last].strip(\"*\")))\n",
    "print(name[0:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = { 'author': name, 'text': text2000}\n",
    "df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>The Project Gutenberg EBook of Lincoln's First...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Various</td>\n",
       "      <td>﻿The Project Gutenberg EBook of The King James...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charles Dodgson AKA Lewis Carroll</td>\n",
       "      <td>The Project Gutenberg EBook of Through the Loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lewis Carroll</td>\n",
       "      <td>The Project Gutenberg EBook of The Hunting of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United States. Central Intelligence Agency</td>\n",
       "      <td>The Project Gutenberg EBook of The 1990 CIA Wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       author  \\\n",
       "0                             Abraham Lincoln   \n",
       "1                                     Various   \n",
       "2           Charles Dodgson AKA Lewis Carroll   \n",
       "3                               Lewis Carroll   \n",
       "4  United States. Central Intelligence Agency   \n",
       "\n",
       "                                                text  \n",
       "0  The Project Gutenberg EBook of Lincoln's First...  \n",
       "1  ﻿The Project Gutenberg EBook of The King James...  \n",
       "2  The Project Gutenberg EBook of Through the Loo...  \n",
       "3  The Project Gutenberg EBook of The Hunting of ...  \n",
       "4  The Project Gutenberg EBook of The 1990 CIA Wo...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#update a few of the noticeable errors. Technically, I shouldn't handle it this way since the files may update at any time. Perhaps I'll just leave it as is. :/\n",
    "df['author'][1] = 'Various'\n",
    "df.author[1998] = 'Fannie Isabelle Sherrick'\n",
    "#Clean the text, like for authors.\n",
    "df['text'].apply(lambda x: text_cleaner(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>The Project Gutenberg EBook of Lincoln's First...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Various</td>\n",
       "      <td>﻿The Project Gutenberg EBook of The King James...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charles Dodgson AKA Lewis Carroll</td>\n",
       "      <td>The Project Gutenberg EBook of Through the Loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lewis Carroll</td>\n",
       "      <td>The Project Gutenberg EBook of The Hunting of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United States. Central Intelligence Agency</td>\n",
       "      <td>The Project Gutenberg EBook of The 1990 CIA Wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       author  \\\n",
       "0                             Abraham Lincoln   \n",
       "1                                     Various   \n",
       "2           Charles Dodgson AKA Lewis Carroll   \n",
       "3                               Lewis Carroll   \n",
       "4  United States. Central Intelligence Agency   \n",
       "\n",
       "                                                text  \n",
       "0  The Project Gutenberg EBook of Lincoln's First...  \n",
       "1  ﻿The Project Gutenberg EBook of The King James...  \n",
       "2  The Project Gutenberg EBook of Through the Loo...  \n",
       "3  The Project Gutenberg EBook of The Hunting of ...  \n",
       "4  The Project Gutenberg EBook of The 1990 CIA Wo...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='author', ylabel='count'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data=df, x='author')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td></td>\n",
       "      <td>***The Project Gutenberg Etext of The Hymns of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td></td>\n",
       "      <td>**The Project Gutenberg Etext of The Crowd, by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td></td>\n",
       "      <td>**The Project Gutenberg Etext of The Psycholog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td></td>\n",
       "      <td>****The Project Gutenberg Etext of Pathology o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td></td>\n",
       "      <td>***The Project Gutenberg Etext of \"The Breitma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1836</th>\n",
       "      <td></td>\n",
       "      <td>\\r\\n\\r\\n\\r\\n\\r\\n******************************...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td></td>\n",
       "      <td>**This is a COPYRIGHTED Project Gutenberg Etex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td></td>\n",
       "      <td>***Project Gutenberg Etext: The Circus Boys Ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td></td>\n",
       "      <td>***Project Gutenberg Etext: The Circus Boys In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td></td>\n",
       "      <td>**The Project Gutenberg Etext of Moby Dick, by...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     author                                               text\n",
       "329          ***The Project Gutenberg Etext of The Hymns of...\n",
       "352          **The Project Gutenberg Etext of The Crowd, by...\n",
       "355          **The Project Gutenberg Etext of The Psycholog...\n",
       "356          ****The Project Gutenberg Etext of Pathology o...\n",
       "361          ***The Project Gutenberg Etext of \"The Breitma...\n",
       "...     ...                                                ...\n",
       "1836         \\r\\n\\r\\n\\r\\n\\r\\n******************************...\n",
       "1897         **This is a COPYRIGHTED Project Gutenberg Etex...\n",
       "1986         ***Project Gutenberg Etext: The Circus Boys Ac...\n",
       "1987         ***Project Gutenberg Etext: The Circus Boys In...\n",
       "1996         **The Project Gutenberg Etext of Moby Dick, by...\n",
       "\n",
       "[136 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.author == '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have 136 rows of unknown authors. I will assign these to a hold out variable that could, potentially, be used to extract authors using a machine learning model.\n",
    "\n",
    "I shall call the variable - holdout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td></td>\n",
       "      <td>***The Project Gutenberg Etext of The Hymns of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td></td>\n",
       "      <td>**The Project Gutenberg Etext of The Crowd, by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td></td>\n",
       "      <td>**The Project Gutenberg Etext of The Psycholog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td></td>\n",
       "      <td>****The Project Gutenberg Etext of Pathology o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td></td>\n",
       "      <td>***The Project Gutenberg Etext of \"The Breitma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author                                               text\n",
       "329         ***The Project Gutenberg Etext of The Hymns of...\n",
       "352         **The Project Gutenberg Etext of The Crowd, by...\n",
       "355         **The Project Gutenberg Etext of The Psycholog...\n",
       "356         ****The Project Gutenberg Etext of Pathology o...\n",
       "361         ***The Project Gutenberg Etext of \"The Breitma..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout = df[df.author == ''].copy()\n",
    "holdout.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.author.replace('', float(\"NaN\"), True)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the authors that are reading as excessively long and update their names so that they register as authors without the extra information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Honore de Balzac', 'Richard Harding Davis', 'Thomas Carlyle', 'Plato',\n",
       "       'Joseph Conrad', 'Jack London', 'Various', 'Henry James',\n",
       "       'Arthur Conan Doyle', 'Charles Dickens',\n",
       "       'Shakespeare PG has multiple editions of William Shakespeare's Complete Works',\n",
       "       'Anonymous', 'Edgar Rice Burroughs', 'Jerome K. Jerome', 'Bret Harte',\n",
       "       'L. Frank Baum', 'Wilkie Collins', 'Victor Appleton',\n",
       "       'Robert Louis Stevenson', 'Xenophon', 'Unknown',\n",
       "       'Mary Roberts Rinehart', 'Rudyard Kipling', 'H. G. Wells',\n",
       "       'Jules Verne', 'Nathaniel Hawthorne', 'Kate Douglas Wiggin',\n",
       "       'Zane Grey', 'Edith Wharton', 'Booth Tarkington',\n",
       "       'Frances Hodgson Burnett', 'Max Beerbohm', 'Alice Meynell',\n",
       "       'Andrew Lang', 'Robert Nemiroff and Jerry Bonnell',\n",
       "       'E. Phillips Oppenheim', 'Sir Walter Scott', 'Owen Wister',\n",
       "       'George MacDonald', 'O. Henry', 'Thomas Babington Macaulay',\n",
       "       'Thomas Bailey Aldrich', 'Thomas Hardy', 'George Bernard Shaw',\n",
       "       'B. M. Bower', 'David Graham Phillips', 'Jane Austen',\n",
       "       'H. Rider Haggard', 'Washington Irving', 'Virgil', 'Sax Rohmer',\n",
       "       'Charles Darwin', 'William Makepeace Thackeray', 'Leo Tolstoy',\n",
       "       'Anthony Trollope', 'John Milton', 'William Shakespeare',\n",
       "       'Lucy Maud Montgomery', 'United States', 'Joseph C. Lincoln',\n",
       "       'Benedict de Spinoza', 'United States. Central Intelligence Agency.',\n",
       "       'E. Nesbit', 'G. K. Chesterton', 'Edna Ferber', 'Charles Kingsley',\n",
       "       'William Dean Howells', 'Edward Gibbon', 'Oscar Wilde', 'Willa Cather',\n",
       "       'Henry Smith Williams', 'Gene Stratton-Porter', 'William MacLeod Raine',\n",
       "       'James Fenimore Cooper', 'Henry Lawson', ', Charles A. Eastman',\n",
       "       'Olive Schreiner', 'Horatio Alger, Jr.', 'G.K. Chesterton',\n",
       "       'Stanley Weyman', 'Emile Gaboriau', 'Benedict of Spinoza',\n",
       "       'Herbert George Wells',\n",
       "       'Robert Louis Stevenson, Edited by Sidney Colvin', 'Anton Checkov',\n",
       "       'Martin Luther', 'Dante Alighieri', 'Howard Pyle', 'Frank Norris',\n",
       "       'E. W. Hornung', 'Horatio Alger', 'Eleanor H. Porter',\n",
       "       'Stewart Edward White', 'George Eliot', 'Henry van Dyke',\n",
       "       'Sara Teasdale', 'George Borrow', 'Robert W. Service', 'J. M. Synge',\n",
       "       'Edgar Allan Poe', 'Daniel Defoe',\n",
       "       'Shakespeare PG has multiple editions of William Shakespeare's Complete Works Copyright law',\n",
       "       'Vachel Lindsay', 'Amy Lowell', 'Gustave Flaubert', 'Samuel Smiles',\n",
       "       'Edna Lyall', 'Thornton W. Burgess', 'Samuel Butler',\n",
       "       'Frederick Douglass', 'Irvin S. Cobb', 'J. M. Barrie',\n",
       "       'Joseph Sheridan Le Fanu', 'Ernest Bramah', 'Albert Payson Terhune',\n",
       "       'Gaston Leroux', 'Stephen Crane', 'Alexander Whyte', 'Don Marquis',\n",
       "       'Kenneth Grahame', 'Andrew Barton 'Banjo' Paterson', 'Alphonse Daudet',\n",
       "       'Gene Stratton Porter', 'Anna Katharine Green', 'Anton Chekhov',\n",
       "       'P. G. Wodehouse', 'Charles Reade', 'Rebecca Harding Davis',\n",
       "       'William Morris', 'Herbert A. Giles', 'Ambrose Bierce',\n",
       "       'Rafael Sabatini', 'Henrik Ibsen', 'Edwin Arlington Robinson',\n",
       "       'John Buchan', 'Edward Bulwer Lytton', 'Charles Mackay',\n",
       "       'Jean Armour Polly', 'Algernon Charles Swinburne', 'Edgar A. Guest',\n",
       "       'Frank R. Stockton',\n",
       "       'Shakespeare PG has multiple editions of William Shakespeare's Complete Works Copyright laws are ch',\n",
       "       'Henry Wadsworth Longfellow', 'Maria Edgeworth', 'Francis Thompson',\n",
       "       'Virginia Woolf', 'Tobias Smollett', 'Aristotle',\n",
       "       'James Whitcomb Riley Copyright laws are changin',\n",
       "       'Shakespeare PG has multiple editions of William Shakespeare's Complete Works Copyright laws are changin',\n",
       "       'Grace Isabel Colbron and Augusta Groner', 'Anthony Hope',\n",
       "       'Elia W. Peattie',\n",
       "       'Susan Fenimore Cooper Volume Copyright laws are changing all over the world, be sure to check the co',\n",
       "       'John Fiske', 'Isaac Taylor Headland', 'Amelia E. Barr',\n",
       "       'Mary E. Wilkins Freeman', 'Charles Brockden Brown', 'Andrew Steinmetz',\n",
       "       'Edna St. Vincent Millay', 'Horatio Alger Jr.', 'George Meredith',\n",
       "       'Ernest Thompson Seton', 'Jean Webster', 'John Galt',\n",
       "       'Sara Cone Bryant',\n",
       "       'Shakespeare PG has multiple editions of William Shakespeare's Complete Works Copyright laws are c',\n",
       "       'Saki', 'Thomas Henry Huxley', 'Susan Fenimore Cooper',\n",
       "       'Richard Jefferies', 'Lytton Strachey', 'Anatole France',\n",
       "       'Ulysses S. Grant', 'Ellis Parker Butler',\n",
       "       'Shakespeare PG has multiple editions of William Shakespeare's Complete Works Copyright l',\n",
       "       'Frank L. Packard', 'Oliver Wendell Holmes, Jr.', 'Augusta Groner',\n",
       "       'Robert Louis Stevenson and Lloyd Osbourne', 'Harold Frederic', 'Japan',\n",
       "       'Frank Stockton',\n",
       "       'Thackeray # in our series by William Makepeace Thackeray Copyright laws are changing al',\n",
       "       'Donald Ogden Stewart', 'Thomas Wentworth Higginson',\n",
       "       'United States Bureau of the Census',\n",
       "       'Louis Ginzberg Copyright laws are changing all over the world, be sure to ch',\n",
       "       'W. Somerset Maugham', 'Laura Lee Hope', 'Lewis Carroll',\n",
       "       'Mark Twain (Samuel Clemens)', 'Louisa May Alcott', 'Mark Twain',\n",
       "       'Herodotus', 'Sophocles', 'An Imitator of Plato', 'Maxim Gorky',\n",
       "       'United States Central Intelligence Agency', 'Bram Stoker',\n",
       "       'Walter Pater', 'Christopher Morley', 'Ludwig van Beethoven',\n",
       "       'William Blake', 'John Greenleaf Whittier', 'Theodore Dreiser',\n",
       "       'Alexis de Toqueville', 'Charles Dickens, et al', 'William Wells Brown',\n",
       "       'or about Honore de Balzac Copyright laws are changing al',\n",
       "       'Harriet E. Wilson', 'Zitkala-Sa', 'Mine, by Margaret Mayo',\n",
       "       'Louis de Rougemont',\n",
       "       'Dumont Copyright laws are changing all over the world, be sure to check the copyrigh',\n",
       "       'George Smith', 'Emma Goldman', 'John Filson', 'Lord Dunsany',\n",
       "       'Charles Babbage', 'B.M. Sinclair, AKA B.M. Bower', 'H. B. Irving',\n",
       "       '(AKA Ralph Iron) Olive Schreiner', 'Rupert Brooke', 'Joe Hutsko',\n",
       "       'James Boswell', 'Henry Benjamin Wheatley', 'Giambattista Basile',\n",
       "       'oject Gutenberg Etext of Autobiography of Andrew Dickson White # in our series Andrew Dickson White See also: Warfare of Science/Theology, An',\n",
       "       'Robert Louis Stevenson, Illustrated by Walter Crane',\n",
       "       'Edmund Burke, Edited by Henry Morley', 'Gail Hamilton', 'E. F. Benson',\n",
       "       'Kipling # in our series by Rudyard Kipling Copyright laws are changing all over the w',\n",
       "       'Gideon Wurdz', 'Alice Freeman Palmer', 'Victor Cherbuliez',\n",
       "       'Ivan S. Turgenev', 'Abraham Lincoln', 'Joseph E. Badger, Jr.',\n",
       "       'Charles W. Chesnutt', 'Plutarch', 'Richard Lovelace',\n",
       "       'Eliza Burt Gamble', 'Mary Wollstonecraft', 'John Fox, Jr.',\n",
       "       'Richard Brinsley Sheridan', 'Enrico Ferri', 'Stan Kerr',\n",
       "       'Norman Coombs', 'Robert Louis Stevenson, et al', 'Margaret Sanger',\n",
       "       'United States. Central Intelligence Agency', 'John Masefield',\n",
       "       'Ellen Robena Field', 'Omar Khayyam', 'John McCrae',\n",
       "       'oject Gutenberg Etext The Lock and Key Library, Hawthorne, Ed. Copyright laws are changing all over the world, be sure to check the copyrigh',\n",
       "       'A.C. Seward and Others',\n",
       "       'Beatrix Potter Copyright laws are changing all over the worl',\n",
       "       'Eleanor Farjeon', 'T. S. Eliot',\n",
       "       'William and Ellen Craft Copyright laws are changing all over the world, be s',\n",
       "       'Diane and Don Nafis, dnafis@nazlo.com The Project Gutenberg Etext of James Otis The Pre-Revolutionist by John Clark Ridpath Co',\n",
       "       'United States National Aeronautics and Space Administration',\n",
       "       'Harriet Beecher Stowe', 'Lucretius',\n",
       "       'oject Gutenberg Etext Use and Need of the Life of C. A. Nation Copyright laws are changing all over the world, be sure to check the copyrigh',\n",
       "       'Margaret Deland', 'Helen Bannerman', 'Isabella L. Bird', 'E. V. Lucas',\n",
       "       'William Dobein James', 'Benjamin Franklin', 'William J. Long',\n",
       "       'John Charles McNeill',\n",
       "       'Justus Friedrich Karl Hecker, Edited by Henry Morley, Translated by Benjami',\n",
       "       'Thomas Papanikolaou', 'Sun Tzu',\n",
       "       'Thomas Alexander Browne, AKA Rolf Boldrewood',\n",
       "       'Thackeray # in our series by William Makepeace Thackeray Copyright laws are changing a',\n",
       "       'Ferdinand Ossendowski', 'W. H. Hudson',\n",
       "       'on And Egypt, by Leonard W. King', 'David Magie Cory',\n",
       "       'Mary Eleanor Wilkins Freeman',\n",
       "       'Mary Esther Miller MacGregor, AKA Marion Keith', 'Walter Raleigh',\n",
       "       'Burbank L. Todd', 'Frank Bird Linderman', 'George Gissing',\n",
       "       'William Alexander Linn', 'Greg Fee',\n",
       "       'Bishop Copyright laws are changing all over the world, be sure to check the copyrigh',\n",
       "       'Oliver Goldsmith', 'Brendan P. Kehoe', 'John Ruskin.',\n",
       "       'Christopher Marlowe', 'James Stephens', 'Scott Hemphill',\n",
       "       'Julian Hawthorne PART I DETECTIVE STORI',\n",
       "       'William Roscoe Thayer Copyright laws are changing all over the world, be s',\n",
       "       'Julian Hawthorne Copyright laws are changing all over the world, b',\n",
       "       'George Borrow, Edited by T. H. Darlow', 'Maurice LeBlanc',\n",
       "       'Elizabeth Gaskell # in our series by Elizabeth Gaskell Copyright laws are changing all over th',\n",
       "       'William Hazlitt', 'Charles and Mary Lamb', 'Alfred Tennyson',\n",
       "       'S. A. Reilly',\n",
       "       'John Lord Copyright laws are changing all over the world. Be sure t',\n",
       "       'Wagner/Burgoyne The author catalog entries are rather strange, so we included an assortment of inf',\n",
       "       'Smyrnaeus Quintus', 'E. R. Punshon', 'Fa-Hsien', 'Hendrik van Loon',\n",
       "       'e Project Gutenberg Etext of An Anthology of Australian Verse Copyright laws are changing all over the world, be sure to check the copyright',\n",
       "       'e Project Gutenberg EBook of Remarks of Mr. Calhoun of South Carolina on the bill to prevent the interference of certain federal officers in elect',\n",
       "       'John Philip Sousa', 'William Beckford, Edited by Henry Morley',\n",
       "       'Guy de Maupassant',\n",
       "       'e Project Gutenberg Etext of Popular Science Monthly Volume Copyright laws are changing all over the world, be sure to check the copyrig',\n",
       "       'e Project Gutenberg Etext of The Narrative of Sojourner Truth',\n",
       "       'Anna Sewell', 'Madame de Lafayette', 'Nettie Garmer Barker',\n",
       "       'Francis Parkman, Jr.', 'English Authors: Orient, by Various',\n",
       "       'William Godwin', 'Dorothy Kilner', 'John Webster', 'Ed Krol',\n",
       "       'Sherwood Anderson', 'Percival Lowell', 'Maynard Barbour',\n",
       "       'August Niemann', 'John Bunyan', 'E. H. Chapin', 'Eliot Gregory',\n",
       "       'A. O. Brownson', 'Charles Dickens, Illustrated by F. H. Townsend',\n",
       "       'Martin Luther # in our series by Martin Luther Copyright laws are changing all over the wor',\n",
       "       'Charles Whibley', 'Electronic Frontier Foundation',\n",
       "       'Johann Wolfgang von Goethe', 'English Authors: Africa, by Various',\n",
       "       'By Kate Douglas Wiggin', 'Owen Meredith', 'Noah Brooks',\n",
       "       '(may be spurious) Plato', 'Philip Melancthon',\n",
       "       'Grace Isabel Colbron, and Augusta Groner', 'Robert Hichens',\n",
       "       'Jack London, Illustrated by Charles Livingston Bull',\n",
       "       'oject Gutenberg's Etext of The Life of General Francis Marion',\n",
       "       'John Dewey', 'European Union', 'Alexandre Dumas, Pere',\n",
       "       'Winn Schwartau', 'Adelaide Anne Procter', 'Project Gutenberg',\n",
       "       'Christopher Evans', 'Edwin L. Arnold', 'Mrs. Mary Rowlandson',\n",
       "       'Torquato Tasso',\n",
       "       'William Shakespeare Copyright laws are changing all over the world. Be sure to chec',\n",
       "       'Herbert Darling Foster', 'Mary Austin',\n",
       "       'onia and Assyria, by Theophilus G. Pinches', 'E. Charles Vivian',\n",
       "       'Conrad Aiken', 'Eden Phillpotts', 'William H. Prescott',\n",
       "       'Carl von Clausewitz',\n",
       "       'J. Henri Fabre, Translated by Alexander Teixeira de Mattos',\n",
       "       'Frank Frankfort Moore', 'Henry Van Dyke', 'Joyce Kilmer',\n",
       "       'Thomas Holmes',\n",
       "       'Benjamin Rumford Copyright laws are changing all over the world, be sure to',\n",
       "       'Emile Zola', 'Chretien DeTroyes', 'E. S. Brooks', 'Arthur Ransome',\n",
       "       'David Livingstone', 'Robert F. Murray, Edited by Andrew Lang',\n",
       "       'James J. Davis', 'Lucius Apuleius', 'Thomas Hughes', 'Sydney Waterlow',\n",
       "       '(AKA Kate Douglas Riggs) Kate Douglas Wiggin', 'Cal Stewart',\n",
       "       'Gary Mann', 'Robert Graves', 'C. J. Cutcliffe Hyne', 'Ross Kay',\n",
       "       'M. Y. Lermontov', 'Leo Tolstoi', 'Alfred, Lord Tennyson',\n",
       "       'Bruce Sterling', 'R. M. Ballantyne, Illustrated by Dalziel',\n",
       "       'Friedrich Schiller', 'Gilbert Keith Chesterton', 'George Sand',\n",
       "       'Edgar B P Darlington Copyright laws', 'Julian Hawthorne',\n",
       "       'Edmond Rostand', 'George M. Gould and Walter Lytle Pyle',\n",
       "       'Andrew Lang, Edited by W. H. Davenport Adams', 'Thomas Jefferson',\n",
       "       'H. Rider Haggard # in our series by H. Rider Haggard',\n",
       "       'Feodor Dostoevsky', 'John Quincy Adams',\n",
       "       'Tilden Copyright laws are changing all over the world, be sure to check the copyrigh',\n",
       "       'Oliver Optic', 'Thomas Nelson Page', 'Myrtle Reed',\n",
       "       'Jerome Klapka, AKA Jerome K. Jerome', 'Lucy Larcom',\n",
       "       'John Lord Copyright laws are changing all over the world. Be sure',\n",
       "       'Mitzi Perdue', 'An Anonymous Young Girl', 'Homer', 'Walter Scott',\n",
       "       'Benjamin Cardozo', 'Edward Lear', 'Thomas Hart Benton',\n",
       "       'Robert Louis Stevenson and Lloyde Osbourne', 'Cicero',\n",
       "       'Natalie Sumner Lincoln', 'Henry Timrod', 'Margaret E. Sangster',\n",
       "       'Daniel Defoe, Edited by Henry Morley', 'William Smith',\n",
       "       'Gilbert K. Chesterton', 'St. Jude, R. U. Sirius, Bart Nagel',\n",
       "       'Frances Jenkins Olcott',\n",
       "       'Geoffrey Chaucer Copyright laws are changing all over the world, be sure t',\n",
       "       'Herman Melville', 'Ruth M. Sprague', 'A. J. O'Reilly',\n",
       "       'Edmund Venables', 'Joanot Martorell and Marti Johan d'Galba',\n",
       "       'Lao-Tze', 'Padraic Colum', 'Ralph Parlette', 'Elbridge S. Brooks',\n",
       "       'Katherine Mansfield', 'Edward Everett Hale', 'Henry David Thoreau',\n",
       "       'Melvin L. Severy', 'B.M. Bower', 'James Hogg',\n",
       "       'Margaret Pollock Sherwood', 'Henry Clay', 'Hjalmar Hjorth Boysen',\n",
       "       'Sylvester Mowry', 'Dr. Martin Luther', 'Vicente Blasco Ibanez',\n",
       "       'Rutherford Platt', 'Stephen Levy', 'Henry W. Longfellow',\n",
       "       'A. A. Milne', 'Rev. James MacCaffrey Copyright laws',\n",
       "       'John N. Reynolds', 'Junction, by Charles Dickens', 'Kate Chopin',\n",
       "       'Elizabeth Gaskell', 'Henry Adams',\n",
       "       'oject Gutenberg Etext Increasing Human Efficiency In Business Copyright laws are changing all over the world, be sure to check the copyright',\n",
       "       'Sinclair Lewis',\n",
       "       'oject Gutenberg Etext of Sir Thomas More ascribed in part to Shakespeare PG has multiple editions of William Shakespeare's Complete Works',\n",
       "       'Etienne Leon Lamothe-Langon',\n",
       "       'e Project Gutenberg EBook of The Holy Bible.', 'Sarojini Naidu',\n",
       "       'Alan Seeger', 'Joshua Reynolds, Edited by Henry Morley',\n",
       "       'Fulgence Marion', 'Helen Cody Wetmore',\n",
       "       'Willa Cather and Alfred Noyes', 'G. Harvey Ralphson',\n",
       "       'Gorky # in our series, by Maxim Gorky Copyright laws are changing all over the wor',\n",
       "       'Frances Hodgson Burnett, Illustrated by F. C. Yohn', 'Louise Muhlbach',\n",
       "       'James M. Barrie', 'Helen Nicolay', 'Wu Tingfang',\n",
       "       'Robert Williams Wood', 'The National Atomic Museum',\n",
       "       'Samuel White Baker', 'Ilya Tolstoy', 'Emma Wolf',\n",
       "       'Samuel Taylor Coleridge', 'John Breckinridge Ellis',\n",
       "       'Sir John Tenniel', 'MICRA, Inc. Copyri', 'LeFebure', 'Henry Fielding',\n",
       "       'Charles Dodgson AKA Lewis Carroll', 'Alfred Lord Tennyson',\n",
       "       'Upton Sinclair and Eugene Brieux', 'Joseph A. Munk',\n",
       "       'Henry James # in our series by Henry James',\n",
       "       'George Borrow, Illustrated by E. J. Sullivan',\n",
       "       'Edward George Bulwer-Lytton', 'Margaret Mayo',\n",
       "       '(AKA Elia Wilkinson) Elia W. Peattie',\n",
       "       'Charles Oliver Copyright laws are changing all over the world, be sure to check the copyright l',\n",
       "       'Hale Copyright laws are changing all over the world, be sure to check the copyright',\n",
       "       'Howard R. Garis',\n",
       "       'J. M. Synge # in our series by J. M. Synge Copyright laws are changing all over the world',\n",
       "       'James Fenimore Cooper # in our series by James Fenimore Cooper Copyright laws are changing all ove',\n",
       "       'by Edward Jenkins', 'Alfred Joyce Kilmer', 'Thomas Love Peacock',\n",
       "       'Thomas Dixon', 'William Congreve, Edited by G. S. Street',\n",
       "       '(His Son) Captain Robert E. Lee', 'Arnold Bennett',\n",
       "       'Miss Mulock Pseudonym of Maria Dinah Craik', 'Arthur Machen',\n",
       "       'John Gower',\n",
       "       'Jack London # - in our series by Jack London Copyright laws are changing all over the wo',\n",
       "       'Arthur B. Reeve', 'Peter Mark Roget', 'Theophile Gautier',\n",
       "       'Edward Carpenter',\n",
       "       'Samuel Adams (# in our series by Samuel Adams) Copyright law',\n",
       "       'Mary Johnston', 'Niccolo Machiavelli', 'Johann Wolfgang Von Goethe',\n",
       "       'H. L. Mencken',\n",
       "       'Carvalho Copyright laws are changing all over the world, be sure to check the copyrigh',\n",
       "       'Shakespeare PG has multiple editions of William Shakespeare's Complete Works Copyrigh',\n",
       "       'John Lawson',\n",
       "       'Neltje Blanchan Copyright laws are changing all over the world, be sure to check the copyrigh',\n",
       "       'Nicolo Machiavelli',\n",
       "       'James Lane Allen Copyright laws are changing all over the world, be sure to check the copyrigh',\n",
       "       'Maria Edgeworth, Edited by Henry Morley',\n",
       "       'Alexander Hamilton and John Jay and James Madison',\n",
       "       'O. Henry, Illustrated by H. C. Greening and May Wilson Preston',\n",
       "       'Richard McGowan', 'Adam Lindsay Gordon', 'Clarence Edgar Johnson',\n",
       "       'Joseph Rodman Drake', 'Thomas More, Edited by Henry Morley',\n",
       "       'John A. Carpenter', 'Phillis Wheatley',\n",
       "       'Joel Chandler Harris Copyright laws are changing all over the world. Be sure to check the copyrigh',\n",
       "       'Edward P. Roe', 'J. Storer Clouston',\n",
       "       'Miller Copyright laws are changing all over the world, be sure to check the copyrigh',\n",
       "       'John Tyndall', 'F. Hopkinson Smith',\n",
       "       'e Project Gutenberg Etext of Stories from Everybody's Magazine From the issues of Everybody's Magazine Copyright laws are changing all o',\n",
       "       'Martin Luther, translated by Robert E. Smith.',\n",
       "       'Jean-Baptiste Poquelin Moliere', 'Max Brand', 'Lewis Roth',\n",
       "       'Avery Hopwood and Mary Roberts Rinehart',\n",
       "       'oject Gutenberg Etext: A Journey in Other Worlds, J. J. Astor Copyright laws are changing all over the world, be sure to check the copyrigh',\n",
       "       'Karl Marx and Friedrich Engels', 'John Dryden',\n",
       "       '(AKA Charlotte, Emily and Anne Bronte) Currer, Ellis, and Acton Bell',\n",
       "       'Victor MacClure', 'Jean Clottes', 'John Fox Jr.',\n",
       "       'Charlotte Perkins Stetson Gilman',\n",
       "       'Don Marquis Copyright laws are changing all over the world',\n",
       "       'Elbert Hubbard', 'Karl Marx',\n",
       "       'Pi (to million digits), by Yasumasa Kanada', 'Alexandre Dumas (Pere)',\n",
       "       'H. Barber', 'Joel Benton. Copyright laws are changing al',\n",
       "       'Adelaide L. Fries', 'Frances E. W. Harper', 'Sextus Propertius',\n",
       "       'White Sands Missile Range Public Affairs Office', 'Robert Harris',\n",
       "       'Chauncey M. Depew', 'Marie Belloc Lowndes', 'Thorstein Veblen',\n",
       "       'Carl Maag and Steve Rohrer', 'Robert Burns',\n",
       "       'Browning, Elizabeth Barrett',\n",
       "       'Cunninghame Graham Copyright laws are changing all over the world, be sure to check the copyright',\n",
       "       'John Murray and Mills Miller', 'Edmund G. Ross',\n",
       "       'Freeman Wills Crofts', 'William Blades', 'Frank T. Bullen',\n",
       "       'Jean Baptiste Racine', 'Frederick A. Talbot', 'George W. Sands',\n",
       "       'Plaatje Copyright laws are changing all over the world, be sure to check the copyright',\n",
       "       'sinia, by Baker Copyright laws are changing all over the world, be sure to check the copyright la',\n",
       "       'C. F. Volney', 'Ayn Rand******* # in our series by Ayn Rand',\n",
       "       'Alexander Hamilton, John Jay, and James Madison', 'Martha Young',\n",
       "       'Calamity Jane', '(AKA Clive Hamilton) C. S. Lewis',\n",
       "       'Thomas A Kempis Copyright laws are changing all over the world, be sure to check the copyright',\n",
       "       'Hesther Lynch Piozzi, Edited by Henry Morley',\n",
       "       'oject Gutenberg Etext of The Dore' Lectures on Mental Science Copyright laws are changing all over the world, be sure to check the copyright',\n",
       "       'e Project Gutenberg Etext \"Divina Commedia di Dante: Inferno\" In Italian with no accents Please see my notes about various versions b',\n",
       "       'Edward S. Ellis et. al.', 'Geoffrey Chaucer', 'Jeffrey Farnol',\n",
       "       'Martha Summerhayes',\n",
       "       'Wm J. Kountz, Jr. Copyright laws are changing all over the world, be sure to check the copyrigh',\n",
       "       'Mrs. Sutherland Orr', 'Apollonius Rhodius', 'M. G. Lewis',\n",
       "       'Elizabeth Cleghorn Gaskell', 'John Fox', 'Valentine Williams',\n",
       "       'Nennius', 'Hugh Latimer, Edited by Henry Morley',\n",
       "       'e Project Gutenberg Etext Divina Commedia di Dante: Purgatorio In Italian with no accents Please see my notes about various versions',\n",
       "       'Dinah Maria Mulock Craik', 'Frances Bacon',\n",
       "       'Jerome K. Jerome, Illustrated by A. Frederics',\n",
       "       'George MacDonald (# in our series by George MacDonald) Copyright laws are changing all over t',\n",
       "       'Donald Mackenzie Wallace', 'Wilfred Owen',\n",
       "       'Adelaide Anne Procter, et al', 'Sir Walter Raleigh',\n",
       "       'Voltaire, Edited by Henry Morley', 'A. B. Paterson', 'James Thomson',\n",
       "       'Bernardin de Saint Pierre', 'Mrs. W. G. Waters', 'Russell H. Conwell',\n",
       "       'Henry Kendall', 'Friedrich Nietzsche',\n",
       "       'Modern American Authors # in our Lock and Key series edited by Julian Hawthorne Copyright laws are c',\n",
       "       'Kakuzo Okakura',\n",
       "       'sinia, by Jerome Lobo, Edited by Henry Morley, Translated by Samuel Johnson',\n",
       "       'S. Weir Mitchell',\n",
       "       'John M. Coulter Copyright laws are changing all over the world, be sure to c',\n",
       "       'Florence Converse', 'Ida Pfeiffer', 'Anna Katherine Green',\n",
       "       'oject Gutenberg Etext of . The Lamentable Tragedy of Locrine . Mucedorus attributed in part to William Shakespeare PG has multiple editio',\n",
       "       'Mr. and Mrs. Haldeman-Julius', 'Homer and Hesiod', 'Francois Rabelais',\n",
       "       'Henry Cabot Lodge, and Theodore Roosevelt', 'Ayn Rand',\n",
       "       'by (AKA B. M. Sinclair) B. M. Bower',\n",
       "       'L. Cranmer-Byng Please take a look at the important information in this header. We encourage you',\n",
       "       'L. Adams Beck', 'Edwin A. Abbot', 'Tadashi Nakashima',\n",
       "       'Hans Christian Andersen', 'Library of Congress Copyright Office',\n",
       "       'Jane Addams Copyright laws are changing all over the world, be sure to check the copyrigh',\n",
       "       'e Project Gutenberg Etext of Organic Syntheses, Conant, Editor Caveat: Some numbers did not OCR correctly and may not have been corrected duri',\n",
       "       'Gertrude Atherton', 'Ruth Ogden', 'William Hickling Prescott',\n",
       "       'Odd de Presno', 'Andrew Lang.', 'B. Perez Galdos', 'Dan Scavelle',\n",
       "       'Chretien de Troyes',\n",
       "       'Jonathan Nield Copyright laws are changing all over the world, be sure to ch',\n",
       "       'Proudhon # in our series by Joseph-Pierre Proudhon Copyright laws are changing all ove',\n",
       "       'Fyodor Dostoyevsky', 'the Lambs', 'John Farrar',\n",
       "       'Richard le Gallienne', 'Elizabeth Drew Stoddard',\n",
       "       'Sara Jeanette Duncan', 'Franklin Delano Roosevelt',\n",
       "       'William Wells Brown ( edition) See Apr Clotelle; or The Colored Hero',\n",
       "       'H. Stanley Redgrove', 'Norman F. Joly',\n",
       "       'Shakespeare PG has multiple editions of William Shakespeare's Complete Works Copyright laws are',\n",
       "       'Howard Trueman Copyright laws are changing all over the world, be sure to c',\n",
       "       'Sarah Orne Jewett', 'Rex Stout', 'Coalition for Networked Information',\n",
       "       'B.M. Sinclair, AKA B. M. Bower', 'Earl Derr Biggers', 'Eugene Field',\n",
       "       'Alice Dunbar', 's, by Edgar Rice Burroughs', 'Sarah Fielding',\n",
       "       'J. Frank Dobie', 'Prosper Merimee', 'Aesop',\n",
       "       'Edgar B. P. Darlington Copyrig', 'Harold MacGrath', 'Giuseppe Salza',\n",
       "       'William D. McClintock and Porter Lander McClintock Copyright laws are chang',\n",
       "       'Herbert N. Casson', 'J. Walker McSpadden', 'Chester K. Steele',\n",
       "       'Oliver Wendell Holmes', 'Robert Service', 'Hiram Corson',\n",
       "       'J. M. Bacon', 'David Herbert Lawrence', 'Alexander H. Japp',\n",
       "       'Eros Urides and J. L. Kennon', 'Boz, by Charles Dickens',\n",
       "       'Arthur Ransome # in our series by Arthur Ransome Copyright laws are changing all over the wo',\n",
       "       'J. Munro', 'Francis Bacon', 'Baruch Spinoza',\n",
       "       'George Wharton James Copyright laws are changing all over the world, b',\n",
       "       'Ellen Glasgow Copyright laws are changing all over the worl',\n",
       "       'Robert Herrick', 'Ellen Key', 'Hjalmar Hjorth Boyesen',\n",
       "       'St. John of Damascus', 'Richard F. Burton',\n",
       "       'Shakespeare PG has multiple editions of William Shakespeare's Complete Works Copyright la',\n",
       "       'Thomas De Quincey', 'Nathaniel Wright Stephenson', 'Gildas',\n",
       "       'James Branch Cabell', 'A. Merritt',\n",
       "       'James Lowell # in our series by James Russell Lowell Copyright laws are changing all over t',\n",
       "       'Sinclair # in our series by Upton Sinclair Copyright laws are changing all over the wor',\n",
       "       'John William Draper', 'Mary Mapes Dodge', 'Jeffery Farnol',\n",
       "       'Bayard Taylor',\n",
       "       'George Iles Copyright laws are changing all over the world, be sure to',\n",
       "       'Richard de Bury', 'Step, by The American Tract Society',\n",
       "       'Jonathan Swift, Edited by Henry Morley', 'Brian Oswald Donn-Byrne',\n",
       "       'Alexandre Dumas, fils', 'L. Frank Baum.',\n",
       "       'Stephen Crane # in our series by Stephen Crane Copyright laws are changing all over the worl',\n",
       "       'Marie L. McLaughlin', 'Thomas Burke', 'The Princess Der Ling',\n",
       "       'Michael Husted', 'Stephen Vincent Benet',\n",
       "       'Shakespeare PG has multiple editions of William Shakespeare's Complete Works Copyright',\n",
       "       'Samuel Johnson', 'by Charles Dickens', 'John Goodwin',\n",
       "       'Beatrix Potter',\n",
       "       'Arthur B. Reeve Copyright laws are changing all over the world, be sure to check the copyrigh',\n",
       "       'Arthur Hugh Clough', 'John Keats',\n",
       "       'Jerome K. Jerome # in our series by Jerome K. Jerome Copyright laws are changing all over th',\n",
       "       'Dornford Yates',\n",
       "       'Smiles # in our series by Samuel Smiles Copyright laws are changing all over the wo',\n",
       "       'William Osler', 'Susanna Rowson',\n",
       "       'Frank Lewis Dyer and Thomas Commerford Martin',\n",
       "       'Shakespeare, PG has multiple editions of William Shakespeare's Complete Works',\n",
       "       'Helen Keller',\n",
       "       'Edwin A. Abbott There are several editions of this ebook in the Project Gutenberg collection. Various',\n",
       "       'Henry Wadsworth Longfellow Copyright laws are changing all ove',\n",
       "       'Unknown Icelanders', 'Alexander Pope, Edited by Henry Morley',\n",
       "       'James Russell Lowell',\n",
       "       'Shakespeare PG has multiple editions of William Shakespeare's Complete Works Copyright laws are chang',\n",
       "       'Margaret Hill McCarter', 'Pierre Loti', 'Miriam Michelson',\n",
       "       'Shakespeare PG has multiple editions of William Shakespeare's Complete Works Copyright laws ar',\n",
       "       'Johanna Spyri =========================================================================== There are sever',\n",
       "       'Upton Sinclair', 'Saxo Grammaticus (\"Saxo the Learned\")',\n",
       "       'Madame de La Fayette',\n",
       "       'Maurice Maeterlinck Copyright laws are changing all over the world, be sure to check the copyright',\n",
       "       'Hilda Conkling',\n",
       "       'U.S. Census of Population and Housing and U.S. Dept. of Commerce, Econ',\n",
       "       'Joseph McCabe', 'R. D. Blackmore',\n",
       "       'James F. Cooper # in our series by James Fenimore Cooper Copyright la',\n",
       "       'William Congreve', 'Franz Josef Haydn', 'Herbert Allen Giles',\n",
       "       'oject Gutenberg Etext of Autobiography of Andrew Dickson White # in our series Andrew Dickson White We are counting both volumes as # in our',\n",
       "       'William Gilmore Simms', 'ENTY YEARS AFTER', 'Michael Hart',\n",
       "       'Captain Marryat # in our series by Captain Marryat Copyright laws are changing all over the w',\n",
       "       'Harry Alonzo Cushing, vol. III. Copyright laws are cha',\n",
       "       'oject Gutenberg Etext of Cromwell attributed in part to Shakespeare PG has multiple editions of William Shakespeare's Complete Works Copyr',\n",
       "       'Charlotte M. Braeme',\n",
       "       'e Project Gutenberg Etext of The Journal of Abnormal Psychology Copyright laws are changing all over the world, be sure to check the copyright',\n",
       "       'United States Army', 'Lodovico Ariosto', 'P. J. Proudhon',\n",
       "       'Mariano Azuela', 'Fannie Isabelle Sherrick', 'Edouard le Roy',\n",
       "       'J. Fitzgerald Molloy', 'Andy Adams',\n",
       "       'William Shakespeare PG has multiple editions of William Shakespeare's Complete Work',\n",
       "       'Robert Southey', 'W.J. Jackman and Thos. H. Russell',\n",
       "       'Mary Murdoch Mason',\n",
       "       'The Project Gutenberg's Etext of Shakespeare's First Folio*** ********************The Tragedie of King Lear*******************',\n",
       "       'Louise de la Ramee (AKA Ouida)', 'E THREE MUSKETEERS',\n",
       "       'Lazarillo of Tormes', '(AKA David Grayson) Ray Stannard Baker',\n",
       "       'J. Stark Munro', 'Amy Steedman', 'William S. Gilbert',\n",
       "       'General William Booth [ The founder of the Salvation Army tells of his philoso',\n",
       "       'Rudge, by Charles Dickens',\n",
       "       'Kate Douglas Smith Wiggin, Illustrated by Claude A. Shepperson',\n",
       "       'Oscar Wilde, Edited by Robert Ross', 'Royall Tyler', 'Marvin Dana',\n",
       "       'Nikolai Vasilievich Gogol',\n",
       "       'Arthur Brisbane # in our series by Arthur Brisbane Copyright laws a',\n",
       "       'R. S. Ball', 'Lew Wallace', 'Joseph A. Altsheler',\n",
       "       'Library of Congress', 'Rene Doumic', 'David Slowinski',\n",
       "       '\"Elizabeth\", AKA Marie Annette Beauchamp',\n",
       "       'Shakespeare PG has multiple editions of William Shakespeare's Complete Works Copyrig',\n",
       "       'United States Arms Control and Disarmament Agency', 'Thoma',\n",
       "       'Long by Helen Beecher Long Copyright laws are changing all over the world, be sure',\n",
       "       'Hall Caine',\n",
       "       'Henry Wadsworth Longfellow Copyright laws are changing all over',\n",
       "       'Jonathan Swift', 'William J. Claxton', 'Lucan', 'Andrew Dickson White',\n",
       "       'Jerry Bonnell and Robert Nemiroff', 'Snorri Sturlason',\n",
       "       'oject Gutenberg Etext of Study of the King James Bible, McAfee',\n",
       "       'Lafcadio Hearn', 'J. E. Hutton', 'Frank Tymon', 'Walter Hawkins',\n",
       "       'George Whale', 'Mark E. Laxer', 'Henry H. Snelling',\n",
       "       'oject Gutenberg Etext of The Reign of King Edward the Third, attributed in part to William Shakespeare. PG has multiple editions of William S',\n",
       "       'Macaulay # in our series by Thomas Babington Macaulay Copyright laws a',\n",
       "       'Epictetus', 'Anna Howard Shaw', 'Edward Bellamy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_seq_items = 1000\n",
    "df.author.value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = ['Hesther Lynch Piozzi','Beatrix Potter','Susan Fenimore Cooper','Baker','Hale','Cunninghame Graham','Charles Oliver', 'William Makepeace Thackeray', \n",
    "               \"Shakespeare\", 'James Russell Lowell', 'Henry Wadsworth Longfellow', 'Andrew Dickson White', 'Ayn Rand', 'Fannie Isabelle Sherrick', 'John Lord', 'James Lane Allen',\n",
    "               'Jack London', 'William Wells Brown', 'Thomas A Kempis', \"Alexander Pope\", 'Louis Ginzberg', 'Jerome K. Jerome', 'Maurice Maeterlinck', 'George Wharton James', \n",
    "                'Rudyard Kipling', 'Jerome Lobo', 'James F. Cooper', 'Charles Dickens', 'Upton Sinclair', 'Johanna Spyri', 'William and Ellen Craft', \n",
    "               'James Whitcomb Riley', 'Helen Beecher Long', 'Joseph-Pierre Proudhon', 'Arthur Brisbane', 'Julian Hawthorne', 'R. M. Ballantyne']\n",
    "for corr in corrections:\n",
    "    df.loc[df.author.str.contains(corr), 'author'] = corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of these and more keep turning up. Manual interpretation didn't work out too well for me and neither did my programmatical magic - so I'm going to hold out on these, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "copyright = df.loc[df.author.str.contains(\"Copyright\")]\n",
    "holdout = pd.concat([copyright, holdout])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.author.str.contains(\"Copyright\")] = float(\"NaN\")\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the bag of words explanation on this part. I need/want to handle the data cleaning up here so we can see it all at once. \n",
    "\n",
    "There are too many sentences to process the bag of words so I need to reduce my author count. It doesn't make sense to include anonymous/unknown/various authors in my training model. How can I train in that scenario? So - remove multiple authors (authors containing the word and), various, editors, unknown, illustrated, aliases (AKA), anyone author with a parenthsis, and anonymous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unknowns, etc.\n",
    "unknowns = [' and ', 'various', 'anonymous', 'unknown', 'enty years after', 'gutenberg', '\\(', 'edited', 'illustrated']\n",
    "various = df.loc[df.author.str.lower().str.contains('|'.join(unknowns))]\n",
    "\n",
    "holdout = pd.concat([various, holdout])\n",
    "\n",
    "#aliases that are labeled with AKA. \n",
    "alias = df.loc[df.author.str.contains('AKA')]\n",
    "\n",
    "holdout = pd.concat([alias, holdout])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.author.str.lower().str.contains('|'.join(unknowns))] = float(\"NaN\")\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.author.str.contains('AKA')] = float(\"NaN\")\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Honore de Balzac         92\n",
       "Shakespeare              52\n",
       "Charles Dickens          27\n",
       "Richard Harding Davis    27\n",
       "Thomas Carlyle           27\n",
       "                         ..\n",
       "Sextus Propertius         1\n",
       "Sara Jeanette Duncan      1\n",
       "Richard le Gallienne      1\n",
       "E. V. Lucas               1\n",
       "Mary Johnston             1\n",
       "Name: author, Length: 665, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.author.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balance out the dataset by holding out on all works greater than the mean; that is, reduce those authors count to three. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_counts = df.author.value_counts()[0:50]\n",
    "keep = author_counts[author_counts.values > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all the works after the first three, because we only want three of them. \n",
    "for a in keep.index:\n",
    "    remove_index = df[df.author.isin([a])][3:].index\n",
    "    remove = df[df.author.isin([a])][3:].copy()\n",
    "    holdout = pd.concat([remove, holdout])\n",
    "    df.drop(remove_index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Robert W. Service</td>\n",
       "      <td>The Project Gutenberg EBook of Ballads of a Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Robert W. Service</td>\n",
       "      <td>Project Gutenberg's Rhymes of a Rolling Stone,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Robert W. Service</td>\n",
       "      <td>Project Gutenberg's Rhymes of a Red Cross Man,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>The Project Gutenberg EBook of Lincoln's First...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lewis Carroll</td>\n",
       "      <td>The Project Gutenberg EBook of The Hunting of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author                                               text\n",
       "192  Robert W. Service  The Project Gutenberg EBook of Ballads of a Ch...\n",
       "238  Robert W. Service  Project Gutenberg's Rhymes of a Rolling Stone,...\n",
       "244  Robert W. Service  Project Gutenberg's Rhymes of a Red Cross Man,...\n",
       "0      Abraham Lincoln  The Project Gutenberg EBook of Lincoln's First...\n",
       "3        Lewis Carroll  The Project Gutenberg EBook of The Hunting of ..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Choose 100 authors\n",
    "# keep = df.author.value_counts()\n",
    "\n",
    "# for a in keep.index:\n",
    "#     remove_index = df[~df.author.isin([a])].index\n",
    "#     remove = df[~df.author.isin([a])].copy()\n",
    "#     holdout = pd.concat([remove, holdout])\n",
    "#     df.drop(remove_index, inplace=True)\n",
    "# holdout.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #I should have done something different. This is a fix to get the data back that I accidentally eradicated from the dataframe. \n",
    "# df = holdout[holdout.author.isin(keep.index)]\n",
    "# holdout = holdout[~holdout.author.isin(keep.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLP generated some kind of an error message when the text exceeded a number of characters. \n",
    "The following steps were taken:\n",
    "> Increased page file size.\n",
    "\n",
    "> Set PC properties to maximize peformance.\n",
    "\n",
    "> Reduced the variable t to translate at that amount. \n",
    "\n",
    "*note: originally I had run nlp against all 2,000 records to set them out as documents. This made it such that the hold out dataframe also contained documents that I could access at a later date. Unfortunately, it took over four hours to process all 2k records, the large dataset caused frequent lagging and I would often have to reboot my computer; slowing down the project. So this step was moved to after the data cleaning process wherein I selected my dataset based on the number of authors.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/12/2020 20:52:03 working on doc 0\n",
      "26/12/2020 20:52:07 working on doc 1\n",
      "26/12/2020 20:52:12 working on doc 2\n",
      "26/12/2020 20:52:17 working on doc 3\n",
      "26/12/2020 20:52:30 working on doc 4\n",
      "26/12/2020 20:52:36 working on doc 5\n",
      "26/12/2020 20:52:50 working on doc 6\n",
      "26/12/2020 20:52:52 working on doc 7\n",
      "26/12/2020 20:53:06 working on doc 8\n",
      "26/12/2020 20:53:19 working on doc 9\n",
      "26/12/2020 20:53:31 working on doc 10\n",
      "26/12/2020 20:53:38 working on doc 11\n",
      "26/12/2020 20:53:44 working on doc 12\n",
      "26/12/2020 20:53:47 working on doc 13\n",
      "26/12/2020 20:53:57 working on doc 14\n",
      "26/12/2020 20:54:09 working on doc 15\n",
      "26/12/2020 20:54:17 working on doc 16\n",
      "26/12/2020 20:54:25 working on doc 17\n",
      "26/12/2020 20:54:26 working on doc 18\n",
      "26/12/2020 20:54:36 working on doc 19\n",
      "26/12/2020 20:54:49 working on doc 20\n",
      "26/12/2020 20:55:01 working on doc 21\n",
      "26/12/2020 20:55:13 working on doc 22\n",
      "26/12/2020 20:55:25 working on doc 23\n",
      "26/12/2020 20:55:38 working on doc 24\n",
      "26/12/2020 20:55:41 working on doc 25\n",
      "26/12/2020 20:55:53 working on doc 26\n",
      "26/12/2020 20:56:06 working on doc 27\n",
      "26/12/2020 20:56:17 working on doc 28\n",
      "26/12/2020 20:56:30 working on doc 29\n",
      "26/12/2020 20:56:43 working on doc 30\n",
      "26/12/2020 20:56:57 working on doc 31\n",
      "26/12/2020 20:57:06 working on doc 32\n",
      "26/12/2020 20:57:20 working on doc 33\n",
      "26/12/2020 20:57:33 working on doc 34\n",
      "26/12/2020 20:57:36 working on doc 35\n",
      "26/12/2020 20:57:44 working on doc 36\n",
      "26/12/2020 20:57:51 working on doc 37\n",
      "26/12/2020 20:57:54 working on doc 38\n",
      "26/12/2020 20:58:07 working on doc 39\n",
      "26/12/2020 20:58:11 working on doc 40\n",
      "26/12/2020 20:58:17 working on doc 41\n",
      "26/12/2020 20:58:19 working on doc 42\n",
      "26/12/2020 20:58:29 working on doc 43\n",
      "26/12/2020 20:58:41 working on doc 44\n",
      "26/12/2020 20:58:46 working on doc 45\n",
      "26/12/2020 20:58:59 working on doc 46\n",
      "26/12/2020 20:59:08 working on doc 47\n",
      "26/12/2020 20:59:22 working on doc 48\n",
      "26/12/2020 20:59:28 working on doc 49\n",
      "26/12/2020 20:59:41 working on doc 50\n",
      "26/12/2020 20:59:45 working on doc 51\n",
      "26/12/2020 20:59:48 working on doc 52\n",
      "26/12/2020 20:59:55 working on doc 53\n",
      "26/12/2020 21:00:10 working on doc 54\n",
      "26/12/2020 21:00:24 working on doc 55\n",
      "26/12/2020 21:00:33 working on doc 56\n",
      "26/12/2020 21:00:38 working on doc 57\n",
      "26/12/2020 21:00:50 working on doc 58\n",
      "26/12/2020 21:01:03 working on doc 59\n",
      "26/12/2020 21:01:15 working on doc 60\n",
      "26/12/2020 21:01:28 working on doc 61\n",
      "26/12/2020 21:01:40 working on doc 62\n",
      "26/12/2020 21:01:42 working on doc 63\n",
      "26/12/2020 21:01:47 working on doc 64\n",
      "26/12/2020 21:02:01 working on doc 65\n",
      "26/12/2020 21:02:11 working on doc 66\n",
      "26/12/2020 21:02:23 working on doc 67\n",
      "26/12/2020 21:02:32 working on doc 68\n",
      "26/12/2020 21:02:42 working on doc 69\n",
      "26/12/2020 21:02:53 working on doc 70\n",
      "26/12/2020 21:03:06 working on doc 71\n",
      "26/12/2020 21:03:12 working on doc 72\n",
      "26/12/2020 21:03:16 working on doc 73\n",
      "26/12/2020 21:03:19 working on doc 74\n",
      "26/12/2020 21:03:20 working on doc 75\n",
      "26/12/2020 21:03:31 working on doc 76\n",
      "26/12/2020 21:03:42 working on doc 77\n",
      "26/12/2020 21:03:53 working on doc 78\n",
      "26/12/2020 21:03:59 working on doc 79\n",
      "26/12/2020 21:04:01 working on doc 80\n",
      "26/12/2020 21:04:06 working on doc 81\n",
      "26/12/2020 21:04:18 working on doc 82\n",
      "26/12/2020 21:04:24 working on doc 83\n",
      "26/12/2020 21:04:30 working on doc 84\n",
      "26/12/2020 21:04:36 working on doc 85\n",
      "26/12/2020 21:04:40 working on doc 86\n",
      "26/12/2020 21:04:53 working on doc 87\n",
      "26/12/2020 21:04:56 working on doc 88\n",
      "26/12/2020 21:05:00 working on doc 89\n",
      "26/12/2020 21:05:03 working on doc 90\n",
      "26/12/2020 21:05:05 working on doc 91\n",
      "26/12/2020 21:05:13 working on doc 92\n",
      "26/12/2020 21:05:15 working on doc 93\n",
      "26/12/2020 21:05:28 working on doc 94\n",
      "26/12/2020 21:05:29 working on doc 95\n",
      "26/12/2020 21:05:30 working on doc 96\n",
      "26/12/2020 21:05:42 working on doc 97\n",
      "26/12/2020 21:05:55 working on doc 98\n",
      "26/12/2020 21:06:00 working on doc 99\n",
      "26/12/2020 21:06:07 working on doc 100\n",
      "26/12/2020 21:06:12 working on doc 101\n",
      "26/12/2020 21:06:14 working on doc 102\n",
      "26/12/2020 21:06:26 working on doc 103\n",
      "26/12/2020 21:06:28 working on doc 104\n",
      "26/12/2020 21:06:30 working on doc 105\n",
      "26/12/2020 21:06:42 working on doc 106\n",
      "26/12/2020 21:06:54 working on doc 107\n",
      "26/12/2020 21:06:56 working on doc 108\n",
      "26/12/2020 21:07:09 working on doc 109\n",
      "26/12/2020 21:07:20 working on doc 110\n",
      "26/12/2020 21:07:27 working on doc 111\n",
      "26/12/2020 21:07:39 working on doc 112\n",
      "26/12/2020 21:07:52 working on doc 113\n",
      "26/12/2020 21:08:03 working on doc 114\n",
      "26/12/2020 21:08:16 working on doc 115\n",
      "26/12/2020 21:08:28 working on doc 116\n",
      "26/12/2020 21:08:36 working on doc 117\n",
      "26/12/2020 21:08:49 working on doc 118\n",
      "26/12/2020 21:08:54 working on doc 119\n",
      "26/12/2020 21:08:58 working on doc 120\n",
      "26/12/2020 21:09:10 working on doc 121\n",
      "26/12/2020 21:09:23 working on doc 122\n",
      "26/12/2020 21:09:24 working on doc 123\n",
      "26/12/2020 21:09:29 working on doc 124\n",
      "26/12/2020 21:09:30 working on doc 125\n",
      "26/12/2020 21:09:32 working on doc 126\n",
      "26/12/2020 21:09:46 working on doc 127\n",
      "26/12/2020 21:09:58 working on doc 128\n",
      "26/12/2020 21:10:10 working on doc 129\n",
      "26/12/2020 21:10:18 working on doc 130\n",
      "26/12/2020 21:10:30 working on doc 131\n",
      "26/12/2020 21:10:42 working on doc 132\n",
      "26/12/2020 21:10:53 working on doc 133\n",
      "26/12/2020 21:11:03 working on doc 134\n",
      "26/12/2020 21:11:15 working on doc 135\n",
      "26/12/2020 21:11:26 working on doc 136\n",
      "26/12/2020 21:11:28 working on doc 137\n",
      "26/12/2020 21:11:36 working on doc 138\n",
      "26/12/2020 21:11:49 working on doc 139\n",
      "26/12/2020 21:12:01 working on doc 140\n",
      "26/12/2020 21:12:08 working on doc 141\n",
      "26/12/2020 21:12:11 working on doc 142\n",
      "26/12/2020 21:12:14 working on doc 143\n",
      "26/12/2020 21:12:22 working on doc 144\n",
      "26/12/2020 21:12:34 working on doc 145\n",
      "26/12/2020 21:12:36 working on doc 146\n",
      "26/12/2020 21:12:40 working on doc 147\n",
      "26/12/2020 21:12:52 working on doc 148\n",
      "26/12/2020 21:12:55 working on doc 149\n",
      "26/12/2020 21:13:03 working on doc 150\n",
      "26/12/2020 21:13:09 working on doc 151\n",
      "26/12/2020 21:13:21 working on doc 152\n",
      "26/12/2020 21:13:24 working on doc 153\n",
      "26/12/2020 21:13:26 working on doc 154\n",
      "26/12/2020 21:13:34 working on doc 155\n",
      "26/12/2020 21:13:34 working on doc 156\n",
      "26/12/2020 21:13:36 working on doc 157\n",
      "26/12/2020 21:13:40 working on doc 158\n",
      "26/12/2020 21:13:45 working on doc 159\n",
      "26/12/2020 21:13:58 working on doc 160\n",
      "26/12/2020 21:14:05 working on doc 161\n",
      "26/12/2020 21:14:14 working on doc 162\n",
      "26/12/2020 21:14:25 working on doc 163\n",
      "26/12/2020 21:14:37 working on doc 164\n",
      "26/12/2020 21:14:40 working on doc 165\n",
      "26/12/2020 21:14:52 working on doc 166\n",
      "26/12/2020 21:15:04 working on doc 167\n",
      "26/12/2020 21:15:07 working on doc 168\n",
      "26/12/2020 21:15:10 working on doc 169\n",
      "26/12/2020 21:15:21 working on doc 170\n",
      "26/12/2020 21:15:33 working on doc 171\n",
      "26/12/2020 21:15:45 working on doc 172\n",
      "26/12/2020 21:15:54 working on doc 173\n",
      "26/12/2020 21:16:06 working on doc 174\n",
      "26/12/2020 21:16:10 working on doc 175\n",
      "26/12/2020 21:16:15 working on doc 176\n",
      "26/12/2020 21:16:26 working on doc 177\n",
      "26/12/2020 21:16:29 working on doc 178\n",
      "26/12/2020 21:16:41 working on doc 179\n",
      "26/12/2020 21:16:46 working on doc 180\n",
      "26/12/2020 21:16:52 working on doc 181\n",
      "26/12/2020 21:17:06 working on doc 182\n",
      "26/12/2020 21:17:12 working on doc 183\n",
      "26/12/2020 21:17:13 working on doc 184\n",
      "26/12/2020 21:17:15 working on doc 185\n",
      "26/12/2020 21:17:18 working on doc 186\n",
      "26/12/2020 21:17:27 working on doc 187\n",
      "26/12/2020 21:17:39 working on doc 188\n",
      "26/12/2020 21:17:49 working on doc 189\n",
      "26/12/2020 21:18:00 working on doc 190\n",
      "26/12/2020 21:18:11 working on doc 191\n",
      "26/12/2020 21:18:24 working on doc 192\n",
      "26/12/2020 21:18:27 working on doc 193\n",
      "26/12/2020 21:18:36 working on doc 194\n",
      "26/12/2020 21:18:48 working on doc 195\n",
      "26/12/2020 21:18:59 working on doc 196\n",
      "26/12/2020 21:19:11 working on doc 197\n",
      "26/12/2020 21:19:24 working on doc 198\n",
      "26/12/2020 21:19:26 working on doc 199\n",
      "26/12/2020 21:19:28 working on doc 200\n",
      "26/12/2020 21:19:38 working on doc 201\n",
      "26/12/2020 21:19:50 working on doc 202\n",
      "26/12/2020 21:19:57 working on doc 203\n",
      "26/12/2020 21:20:04 working on doc 204\n",
      "26/12/2020 21:20:12 working on doc 205\n",
      "26/12/2020 21:20:24 working on doc 206\n",
      "26/12/2020 21:20:37 working on doc 207\n",
      "26/12/2020 21:20:46 working on doc 208\n",
      "26/12/2020 21:20:59 working on doc 209\n",
      "26/12/2020 21:21:11 working on doc 210\n",
      "26/12/2020 21:21:23 working on doc 211\n",
      "26/12/2020 21:21:34 working on doc 212\n",
      "26/12/2020 21:21:44 working on doc 213\n",
      "26/12/2020 21:21:57 working on doc 214\n",
      "26/12/2020 21:22:10 working on doc 215\n",
      "26/12/2020 21:22:12 working on doc 216\n",
      "26/12/2020 21:22:19 working on doc 217\n",
      "26/12/2020 21:22:31 working on doc 218\n",
      "26/12/2020 21:22:34 working on doc 219\n",
      "26/12/2020 21:22:36 working on doc 220\n",
      "26/12/2020 21:22:47 working on doc 221\n",
      "26/12/2020 21:22:50 working on doc 222\n",
      "26/12/2020 21:22:52 working on doc 223\n",
      "26/12/2020 21:22:57 working on doc 224\n",
      "26/12/2020 21:23:01 working on doc 225\n",
      "26/12/2020 21:23:14 working on doc 226\n",
      "26/12/2020 21:23:23 working on doc 227\n",
      "26/12/2020 21:23:27 working on doc 228\n",
      "26/12/2020 21:23:38 working on doc 229\n",
      "26/12/2020 21:23:47 working on doc 230\n",
      "26/12/2020 21:23:58 working on doc 231\n",
      "26/12/2020 21:24:01 working on doc 232\n",
      "26/12/2020 21:24:09 working on doc 233\n",
      "26/12/2020 21:24:24 working on doc 234\n",
      "26/12/2020 21:24:34 working on doc 235\n",
      "26/12/2020 21:24:46 working on doc 236\n",
      "26/12/2020 21:24:56 working on doc 237\n",
      "26/12/2020 21:25:02 working on doc 238\n",
      "26/12/2020 21:25:08 working on doc 239\n",
      "26/12/2020 21:25:14 working on doc 240\n",
      "26/12/2020 21:25:22 working on doc 241\n",
      "26/12/2020 21:25:28 working on doc 242\n",
      "26/12/2020 21:25:29 working on doc 243\n",
      "26/12/2020 21:25:34 working on doc 244\n",
      "26/12/2020 21:25:42 working on doc 245\n",
      "26/12/2020 21:25:50 working on doc 246\n",
      "26/12/2020 21:25:56 working on doc 247\n",
      "26/12/2020 21:26:04 working on doc 248\n",
      "26/12/2020 21:26:11 working on doc 249\n",
      "26/12/2020 21:26:17 working on doc 250\n",
      "26/12/2020 21:26:23 working on doc 251\n",
      "26/12/2020 21:26:36 working on doc 252\n",
      "26/12/2020 21:26:43 working on doc 253\n",
      "26/12/2020 21:26:47 working on doc 254\n",
      "26/12/2020 21:27:00 working on doc 255\n",
      "26/12/2020 21:27:12 working on doc 256\n",
      "26/12/2020 21:27:25 working on doc 257\n",
      "26/12/2020 21:27:29 working on doc 258\n",
      "26/12/2020 21:27:41 working on doc 259\n",
      "26/12/2020 21:27:49 working on doc 260\n",
      "26/12/2020 21:28:02 working on doc 261\n",
      "26/12/2020 21:28:15 working on doc 262\n",
      "26/12/2020 21:28:27 working on doc 263\n",
      "26/12/2020 21:28:39 working on doc 264\n",
      "26/12/2020 21:28:40 working on doc 265\n",
      "26/12/2020 21:28:47 working on doc 266\n",
      "26/12/2020 21:28:59 working on doc 267\n",
      "26/12/2020 21:29:12 working on doc 268\n",
      "26/12/2020 21:29:21 working on doc 269\n",
      "26/12/2020 21:29:33 working on doc 270\n",
      "26/12/2020 21:29:42 working on doc 271\n",
      "26/12/2020 21:29:51 working on doc 272\n",
      "26/12/2020 21:30:01 working on doc 273\n",
      "26/12/2020 21:30:08 working on doc 274\n",
      "26/12/2020 21:30:21 working on doc 275\n",
      "26/12/2020 21:30:27 working on doc 276\n",
      "26/12/2020 21:30:34 working on doc 277\n",
      "26/12/2020 21:30:40 working on doc 278\n",
      "26/12/2020 21:30:48 working on doc 279\n",
      "26/12/2020 21:31:00 working on doc 280\n",
      "26/12/2020 21:31:12 working on doc 281\n",
      "26/12/2020 21:31:24 working on doc 282\n",
      "26/12/2020 21:31:35 working on doc 283\n",
      "26/12/2020 21:31:41 working on doc 284\n",
      "26/12/2020 21:31:45 working on doc 285\n",
      "26/12/2020 21:31:50 working on doc 286\n",
      "26/12/2020 21:32:00 working on doc 287\n",
      "26/12/2020 21:32:06 working on doc 288\n",
      "26/12/2020 21:32:12 working on doc 289\n",
      "26/12/2020 21:32:18 working on doc 290\n",
      "26/12/2020 21:32:26 working on doc 291\n",
      "26/12/2020 21:32:30 working on doc 292\n",
      "26/12/2020 21:32:39 working on doc 293\n",
      "26/12/2020 21:32:46 working on doc 294\n",
      "26/12/2020 21:32:54 working on doc 295\n",
      "26/12/2020 21:33:06 working on doc 296\n",
      "26/12/2020 21:33:16 working on doc 297\n",
      "26/12/2020 21:33:25 working on doc 298\n",
      "26/12/2020 21:33:37 working on doc 299\n",
      "26/12/2020 21:33:49 working on doc 300\n",
      "26/12/2020 21:33:55 working on doc 301\n",
      "26/12/2020 21:34:08 working on doc 302\n",
      "26/12/2020 21:34:21 working on doc 303\n",
      "26/12/2020 21:34:33 working on doc 304\n",
      "26/12/2020 21:34:47 working on doc 305\n",
      "26/12/2020 21:34:49 working on doc 306\n",
      "26/12/2020 21:34:54 working on doc 307\n",
      "26/12/2020 21:34:58 working on doc 308\n",
      "26/12/2020 21:35:02 working on doc 309\n",
      "26/12/2020 21:35:06 working on doc 310\n",
      "26/12/2020 21:35:11 working on doc 311\n",
      "26/12/2020 21:35:16 working on doc 312\n",
      "26/12/2020 21:35:18 working on doc 313\n",
      "26/12/2020 21:35:21 working on doc 314\n",
      "26/12/2020 21:35:24 working on doc 315\n",
      "26/12/2020 21:35:27 working on doc 316\n",
      "26/12/2020 21:35:31 working on doc 317\n",
      "26/12/2020 21:35:34 working on doc 318\n",
      "26/12/2020 21:35:38 working on doc 319\n",
      "26/12/2020 21:35:42 working on doc 320\n",
      "26/12/2020 21:35:46 working on doc 321\n",
      "26/12/2020 21:35:50 working on doc 322\n",
      "26/12/2020 21:35:53 working on doc 323\n",
      "26/12/2020 21:35:57 working on doc 324\n",
      "26/12/2020 21:36:01 working on doc 325\n",
      "26/12/2020 21:36:05 working on doc 326\n",
      "26/12/2020 21:36:10 working on doc 327\n",
      "26/12/2020 21:36:14 working on doc 328\n",
      "26/12/2020 21:36:18 working on doc 329\n",
      "26/12/2020 21:36:22 working on doc 330\n",
      "26/12/2020 21:36:26 working on doc 331\n",
      "26/12/2020 21:36:30 working on doc 332\n",
      "26/12/2020 21:36:31 working on doc 333\n",
      "26/12/2020 21:36:34 working on doc 334\n",
      "26/12/2020 21:36:38 working on doc 335\n",
      "26/12/2020 21:36:44 working on doc 336\n",
      "26/12/2020 21:36:48 working on doc 337\n",
      "26/12/2020 21:36:54 working on doc 338\n",
      "26/12/2020 21:36:59 working on doc 339\n",
      "26/12/2020 21:37:02 working on doc 340\n",
      "26/12/2020 21:37:07 working on doc 341\n",
      "26/12/2020 21:37:10 working on doc 342\n",
      "26/12/2020 21:37:13 working on doc 343\n",
      "26/12/2020 21:37:18 working on doc 344\n",
      "26/12/2020 21:37:22 working on doc 345\n",
      "26/12/2020 21:37:27 working on doc 346\n",
      "26/12/2020 21:37:28 working on doc 347\n",
      "26/12/2020 21:37:28 working on doc 348\n",
      "26/12/2020 21:37:29 working on doc 349\n",
      "26/12/2020 21:37:30 working on doc 350\n",
      "26/12/2020 21:37:33 working on doc 351\n",
      "26/12/2020 21:37:38 working on doc 352\n",
      "26/12/2020 21:37:41 working on doc 353\n",
      "26/12/2020 21:37:45 working on doc 354\n"
     ]
    }
   ],
   "source": [
    "#Now is the time. Turn these text fields into documents.\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = []\n",
    "author = []\n",
    "i = 0\n",
    "df['doc'] = \"\"\n",
    "df['no_stop'] = \"\"\n",
    "# All the processing work takes place below, so it may take a while.\n",
    "for index, t in df.iterrows():\n",
    "    print(f'{dt.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")} working on doc {i}')\n",
    "    #try to process the text file into a document. Skip it if it can't. As many as possible.\n",
    "    try:\n",
    "        #500000 characters seems long enough for this exercise. I just want to identify the author - which is a text field labled \"Author:\" or whoever is after the \"by\" statement. \n",
    "        t['doc'] = nlp(t.text[0:500000])\n",
    "        t['no_stop'] =  [token for token in t.doc if not token.is_stop]\n",
    "        i += 1\n",
    "    except:\n",
    "        i += 1\n",
    "        print(\"encountered an exception, skipping to next document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-b5fbef9602a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauthor\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Honore de Balzac'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'doc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    877\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 879\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1495\u001b[0m             \u001b[1;31m# validate the location\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1496\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1498\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1435\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1436\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1437\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[1;31m# -------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "dist_plot = df[['author', 'doc']].iloc[0]\n",
    "length = [len(w) for w in dist_plot.doc]\n",
    "fdist = nltk.FreqDist(length)\n",
    "fdist.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate your list of the most common words\n",
    "word_freq = word_frequencies(df[df.author == 'Honore de Balzac']['doc'].iloc[0]).most_common(10)\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by sentences for each of these documents. \n",
    "list_sentences = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    list_sentences += ([[sent, row.author, row.doc[0:500]] for sent in row.doc.sents])\n",
    "    \n",
    "# Combine the sentences from the two novels into one DataFrame\n",
    "sentences = pd.DataFrame(list_sentences, columns = [\"sentences\", \"author\", 'title'])\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of stop words and punctuation and lemmatize the tokens\n",
    "for i, sentence in enumerate(sentences[\"sentences\"]):\n",
    "    sentences.loc[i, \"sentences\"] = [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing is taking over 4 hours to complete. It takes about a half hour to complete download the files from the internet. \n",
    "\n",
    "I am exporting the data object using pickle so that I can access the object in case I have to reboot. \n",
    "I've got to run a grid search, yet. \n",
    "\n",
    "I receive the following error when I attempt to pickle the tokens and sentences:\n",
    "```\n",
    "NotImplementedError: [E111] Pickling a token is not supported, because tokens are only views of the parent Doc and can't exist on their own. A pickled token would always have to include its Doc and Vocab, which has practically no advantage over pickling the parent Doc directly. So instead of pickling the token, pickle the Doc it belongs to.\n",
    "```\n",
    "\n",
    "The pickle file, at this juncture (100 authors) , is well over 25 GB, so I'm saving just the doc and the author from the dataframe. \n",
    "\n",
    "Last, but not least, the file is being saved outside of the project folder because it's too big for GitHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/12/2020 09:57:38: pickling dataframes.\n",
      "27/12/2020 10:06:46: done pickling dataframes\n"
     ]
    }
   ],
   "source": [
    "#Pickle the author and doc in case I have to reboot. \n",
    "timer('pickling dataframes.')\n",
    "\n",
    "with open(\"..\\d_store\\data.pkl\", 'wb') as f:\n",
    "    pickle.dump(df[['author', 'doc']], f)\n",
    "\n",
    "timer('done pickling dataframes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well when I was processing at 100 authors, I could write the file, but not read it. \n",
    "\n",
    "I got the following \n",
    "``` MemoryError: Error assigning 4916400 bytes ```\n",
    "\n",
    "I'm looking at the amount of time it takes to load and the number of documents it can process.\n",
    "It's between 354 and 1,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/12/2020 10:06:46: opening the data\n",
      "27/12/2020 10:16:56: done opening the data.\n"
     ]
    }
   ],
   "source": [
    "timer(\"opening the data\")\n",
    "with open(\"..\\d_store\\data.pkl\", 'rb') as f:\n",
    "    blap = pickle.load(f)\n",
    "timer(\"done opening the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(blap.doc.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When pickle timed out at 1,000 documents, I started looking for alternatives. This is what joblib looks like.\n",
    "\n",
    "Joblib got it's first shot at 354 documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/12/2020 10:48:18: saving\n",
      "27/12/2020 11:39:19: done saving\n"
     ]
    }
   ],
   "source": [
    "timer(\"saving\")\n",
    "joblib.dump(df[['author', 'doc']], \"..\\d_store\\data.sav\")\n",
    "timer('done saving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/12/2020 11:39:23: loading\n",
      "27/12/2020 12:11:54: done loading\n"
     ]
    }
   ],
   "source": [
    "timer('loading')\n",
    "blaj = joblib.load(\"..\\d_store\\data.sav\")\n",
    "timer('done loading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(blaj.doc.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bag of words, I have to troubleshoot a memory error. \n",
    "\n",
    "`MemoryError: Unable to allocate 27.0 TiB for an array with shape (6566859, 565300) and data type int64`\n",
    "\n",
    "Step One: Reduce the size by changing the data type.\n",
    "\n",
    "> Another error arises when I attempt to mask the shape to an int8.\n",
    "\n",
    ">```\n",
    "import numpy as np\n",
    "mask = np.zeros(X.shape,dtype='uint8')\n",
    ">```\n",
    "    \n",
    ">`MemoryError: Unable to allocate 3.38 TiB for an array with shape (6566859, 565300) and data type uint8`\n",
    "\n",
    "\n",
    "Step Two: Reduce content by reducing authors.\n",
    "> Project requirements said ten authors and thousands of texts, with at least seven novels. \n",
    "\n",
    "> I returned to the data cleaning process to eliminate various/anonymous/unknown authors.\n",
    "\n",
    "> I still get an error.\n",
    "\n",
    "> `MemoryError: Unable to allocate 16.1 TiB for an array with shape (5928859, 373831) and data type int64`\n",
    "\n",
    "Step Three: Reduce content again by reducing authors in a way such that the dataset is more balanced. \n",
    ">I returned to the data cleaning process to reduce the number of works of those largest authors to three or four. \n",
    "\n",
    ">Received a similar memory error. \n",
    "\n",
    "Step Four: Skip the dataframe, train the model.\n",
    "\n",
    "> Received a similar memory error and the entire system locked up. \n",
    "\n",
    "Step Five: Reduce the number of authors.  \n",
    "> ```Unable to allocate 7.31 GiB for an array with shape (2952352, 665) and data type int32```\n",
    "\n",
    "> Return to data cleaning and choose the top 100 authors. Decrease steadily until the program runs. \n",
    "\n",
    "In the interest of keeping things easy and to the point; I'll be refraining from adding the error messages. The logistic regression was taking an excessive amount of time to run. I attempted to shape the data into a dataframe again and received an error mentioning 2.77 TiB for 100 authors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sentences['author']\n",
    "X = sentences['sentences']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "X_train = vectorizer.fit_transform(X_train.astype(str))\n",
    "X_test = vectorizer.transform(X_test.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.77 TiB for an array with shape (2314536, 164250) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-3a72d573fc7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbow_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbow_sents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbow_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"author\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1183\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.77 TiB for an array with shape (2314536, 164250) and data type int64"
     ]
    }
   ],
   "source": [
    "bow_df = pd.DataFrame(X_train.toarray(), columns=vectorizer.get_feature_names())\n",
    "bow_sents = pd.concat([bow_df, sentences[[\"text\", \"author\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-3da6089501ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Test on logistic regression to see if it runs. Expand from here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1415\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1416\u001b[0m                       sample_weight=sample_weight)\n\u001b[1;32m-> 1417\u001b[1;33m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[0;32m   1418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[0;32m    758\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"L-BFGS-B\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 760\u001b[1;33m                 \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"iprint\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0miprint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"gtol\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"maxiter\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m             )\n\u001b[0;32m    762\u001b[0m             n_iter_i = _check_optimize_result(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m--> 618\u001b[1;33m                                 callback=callback, **options)\n\u001b[0m\u001b[0;32m    619\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[1;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;34m\"\"\" returns the the function value \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfunc\u001b[1;34m(x, *args)\u001b[0m\n\u001b[0;32m    733\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_multi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 735\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0m_multinomial_loss_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    736\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'newton-cg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0m_multinomial_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_multinomial_loss_grad\u001b[1;34m(w, X, Y, alpha, sample_weight)\u001b[0m\n\u001b[0;32m    345\u001b[0m     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),\n\u001b[0;32m    346\u001b[0m                     dtype=X.dtype)\n\u001b[1;32m--> 347\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_multinomial_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m     \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_multinomial_loss\u001b[1;34m(w, X, Y, alpha, sample_weight)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mintercept\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msquared_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Test on logistic regression to see if it runs. Expand from here. \n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(classification_report(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = model_results('bow', X_train.toarray(), X_test.toarray(), y_train, y_test)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some constant variables for parameter settings. \n",
    "#cross validation, constant parameter\n",
    "cv = 5\n",
    "\n",
    "#multiclass\n",
    "mc = ['auto', 'ovr', 'multinomial']\n",
    "#solver\n",
    "sv = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "#penalty\n",
    "py = ['l1', 'l2', 'elasticnet', 'none']\n",
    "#criterion\n",
    "cn = ['gini', 'entropy']\n",
    "#max_features\n",
    "mf = ['auto', 'sqrt', 'log2']\n",
    "#splitter\n",
    "sr = ['best', 'random']\n",
    "#max_depth\n",
    "md = [5, 7, 15, 29] \n",
    "#n_estimators\n",
    "ne = [n+50 for n in range(50, 500, 50)]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression\n",
    "lrp = {    \"lr__multi_class\": mc, \n",
    "           \"lr__solver\": sv,\n",
    "           'lr__penalty' : py,\n",
    "      }\n",
    "#decision tree\n",
    "dtp = {\n",
    "           'dt__criterion': cn,\n",
    "           'dt__max_features': mf,\n",
    "           'dt__splitter':sr, \n",
    "           'dt__max_depth': md     \n",
    "}\n",
    "#K Nearest Neighbors\n",
    "knnp = {\n",
    "           'knn__n_neighbors': [3, 9, 39, 12],\n",
    "           'knn__weights': ['uniform', 'distance'],\n",
    "           'knn__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], \n",
    "           'knn__leaf_size': [30, 12, 9, 3, 17],  \n",
    "           'knn__metric': ['mahalanobis','euclidean', 'manhattan', 'chebyshev', 'minkowski', 'seuclidean'],  \n",
    "}\n",
    "#Support Vector Classifier\n",
    "svp = {\n",
    "          'sv__kernel': ['linear', 'rbf'],\n",
    "          'sv__break_ties': [True, False],\n",
    "}\n",
    "#Random Forest Classifier \n",
    "rfp = {\n",
    "           'rf__criterion': cn,\n",
    "           'rf__max_features': mf,\n",
    "           'rf__oob_score':[True, False], \n",
    "           'rf__max_depth': md,\n",
    "           'rf__n_estimators': ne,     \n",
    "}\n",
    "#gradient boost \n",
    "gbp = {    'gb__criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "           'gb__max_features': mf,\n",
    "           'gb__max_depth': md, \n",
    "           'gb__n_estimators': ne\n",
    "          }\n",
    "transform = { 'vectorizer__analyzer': word }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Update pipeline with parameters and classifiers.\n",
    "pipeline = Pipeline([('vectorizer', CountVectorizer(analyzer='word')), ('nb', ComplementNB())])\n",
    "params = transform\n",
    "search = GridSearchCV(pipeline, params, cv=cv)\n",
    "print(f'{dt.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}, started grid search')\n",
    "search.fit(X_train, y_train)\n",
    "print(f'{dt.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}, grid search complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ",\n",
    "                     ('rf', RandomForestClassifier(random_state=1)),\n",
    "                     ('lr', LogisticRegression()),\n",
    "                     ('gb', GradientBoostingClassifier()),\n",
    "                     ('knn', KNeighborsClassifier()),\n",
    "                     ('sv', SVC()),\n",
    "                     ('dt', DecisionTreeClassifier())\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_model = search.best_estimator_\n",
    "\n",
    "bow_model.fit(X_train, y_train)\n",
    "y_pred = bow_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
