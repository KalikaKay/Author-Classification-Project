{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Classification Project\n",
    "\n",
    "*a thinkful project by Kalika Kay Curry*\n",
    "\n",
    "Build a project to classify text-author.\n",
    "\n",
    "Collect thousand texts from Gutenberg project (and 7 novels) for at least 10 authors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "import pandas as pd\n",
    "import en_core_web_sm\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly.express as px\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation that spaCy doesn't\n",
    "    # recognize: the double dash --. Better get rid of it now!\n",
    "    text = re.sub(r\"\\r?\\n|\\r\", \" \", text)\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_frequencies(text):\n",
    "    length = [len(w) for w in text]\n",
    "    fdist = nltk.FreqDist(length)\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to calculate how frequently words appear in the text\n",
    "def word_frequencies(text):\n",
    "    \n",
    "    # Build a list of words\n",
    "    # Strip out punctuation\n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct:\n",
    "            words.append(token.text)\n",
    "            \n",
    "    # Build and return a `Counter` object containing word counts\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(text):\n",
    "    print(f'{dt.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listtostrings(s):\n",
    "    return ' '.join([str(elem) for elem in s]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/12/2020 19:49:35, started\n",
      "26/12/2020 20:11:42, ended\n"
     ]
    }
   ],
   "source": [
    "#Thousands of Gutenberg texts.\n",
    "print(f'{dt.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}, started')\n",
    "text2000 = []\n",
    "i = 9\n",
    "count = 0\n",
    "while count != 2000:    \n",
    "    try:\n",
    "        url = f'http://www.gutenberg.org/files/{i}/{i}.txt'\n",
    "        response = request.urlopen(url)\n",
    "        raw = response.read().decode('utf8')\n",
    "        text2000.append(raw )\n",
    "        count += 1\n",
    "        i += 1\n",
    "    except:\n",
    "        i += 1\n",
    "print(f'{dt.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}, ended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authors\n",
    "\n",
    "The texts from the Gutenberg library does not provide authors in a seperate field. Even the html files don't contain a seperate author field for identifying, easily, who the work is by; so I can't really run a classifier on this yet. \n",
    "\n",
    "At the same time, I'm curious who these 2,000 works are by - how many of each author do I have? So a little bit of data exploration and preprocessing to get the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From working with gutenberg data in previous examples, I know that most authors appear within the first 150 characters of the text. Usually it's less than 150 characters, somewhere in the 75 character range. I want the first 150 characters because of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The Project Gutenberg EBook of Roget's Thesaurus, by Peter Mark Roget This eBook is for the use of anyone anywhere at no cost and with almost no r\", 'The Project Gutenberg EBook of The Narrative of the Life of Frederick Douglass, by Frederick Douglass This eBook is for the use of anyone anywhe', 'The Project Gutenberg EBook of O Pioneers!, by Willa Cather This eBook is for the use of anyone anywhere at no cost and with almost no restriction', 'The Project Gutenberg EBook of The CIA World Factbook, by United States. Central Intelligence Agency. This eBook is for the use of anyone a', 'The Project Gutenberg EBook of Paradise Lost, by John Milton ******************************************************************* THIS EBOOK WAS']\n"
     ]
    }
   ],
   "source": [
    "#Authors names usually appear within the first 150 characters. \n",
    "authors = []\n",
    "for text in text2000:\n",
    "    authors.append(text_cleaner(text[0:150]))\n",
    "print(authors[10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abraham', 'Charles', 'Lewis', 'United', 'James', 'Anonymous', 'Alexander', 'Henry', 'John', 'Peter', 'Frederick', 'Willa', 'United', 'John', 'Thomas']\n"
     ]
    }
   ],
   "source": [
    "#Authors will appears after the word \"by\"\n",
    "tokens = nltk.word_tokenize(listtostrings(authors))\n",
    "authortext = nltk.Text(tokens)\n",
    "\n",
    "#abandoing this method for - reasons.\n",
    "first = nltk.text.ConcordanceIndex(authortext).offsets('by') \n",
    "example = []\n",
    "for x in first:\n",
    "    example.append(authortext[x+1])\n",
    "print(example[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Looking at how often the words Gutenberg and by appear in the text.\n",
    "authortext.dispersion_plot(['by', 'Gutenberg', 'Anonymous', 'Various', 'This', 'Author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 2007 matches:\n",
      "The Project Gutenberg EBook of Lincoln 's First Inaugura\n",
      " of anyone anywhere a ﻿The Project Gutenberg EBook of The King James Bible * * \n",
      " * EBOOK ( # ) WAS ONE The Project Gutenberg EBook of Through the Looking-Glass\n",
      "eBook is for the use o The Project Gutenberg EBook of The Hunting of the Snark \n",
      "o cost and with almost The Project Gutenberg EBook of The CIA World Factbook , \n",
      "r the use of anyone an The Project Gutenberg EBook of Peter Pan , by James M. B\n",
      "h almost no restrictio The Project Gutenberg EBook of The Book Of Mormon , by A\n",
      "with almost no restric The Project Gutenberg EBook of The Federalist Papers , b\n",
      "r the use of anyone an The Project Gutenberg EBook of The Song Of Hiawatha , by\n",
      "no cost and with almos The Project Gutenberg EBook of Paradise Lost , by John M\n",
      "h almost no restrictio The Project Gutenberg EBook of Roget 's Thesaurus , by P\n",
      "t and with almost no r The Project Gutenberg EBook of The Narrative of the Life\n",
      "e use of anyone anywhe The Project Gutenberg EBook of O Pioneers ! , by Willa C\n",
      " almost no restriction The Project Gutenberg EBook of The CIA World Factbook , \n",
      "or the use of anyone a The Project Gutenberg EBook of Paradise Lost , by John M\n",
      "* * * * THIS EBOOK WAS The Project Gutenberg EBook of Far from the Madding Crow\n",
      " * * * * * * * * * THI The Project Gutenberg EBook of Aesop 's Fables , by Aeso\n",
      "ost no restrictions wh The Project Gutenberg EBook of Census of Population and \n",
      "k is for the use of anyone Project Gutenberg 's The Bible , King James Version \n",
      "ost and with almost no The Project Gutenberg EBook of The Oedipus Trilogy , by \n",
      " with almost no restri The Project Gutenberg EBook of Herland , by Charlotte Pe\n",
      "no cost and with almos The Project Gutenberg EBook of The Scarlet Letter , by N\n",
      "at no cost and with almost Project Gutenberg 's Zen and the Art of the Internet\n",
      "cost and with almost n The Project Gutenberg EBook of The War of the Worlds , b\n",
      " and with almost no re The Project Gutenberg EBook of Census of Population and \n"
     ]
    }
   ],
   "source": [
    "authortext.concordance('Gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abraham Lincoln', 'he Project Gutenberg EBook of The King James Bible', 'Charles Dodgson AKA Lewis Carroll', 'Lewis Carroll', 'United States. Central Intelligence Agency', 'James M. Barrie', 'Anonymous', 'Alexander Hamilton and John Jay and James Madison', 'Henry W. Longfellow', 'John Milton', 'Peter Mark Roget', 'Frederick Douglass', 'Willa Cather', 'United States. Central Intelligence Agency.', 'John Milton', 'Thomas Hardy', 'Aesop', 'United States Bureau of the Census', 'Various', 'Sophocles', 'Charlotte Perkins Stetson Gilman', 'Nathaniel Hawthorne', 'Brendan P. Kehoe', 'H. G. Wells', 'United States Bureau of the Census']\n"
     ]
    }
   ],
   "source": [
    "#This is the best that I can do. \n",
    "name = []\n",
    "for x in authors:\n",
    "    first = x.find('by') + len('by ')\n",
    "    last = x.find('This') \n",
    "    if last == -1:\n",
    "        last = x.find('*')\n",
    "    name.append(text_cleaner(x[first:last].strip(\"*\")))\n",
    "print(name[0:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = { 'author': name, 'text': text2000}\n",
    "df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>The Project Gutenberg EBook of Lincoln's First...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Various</td>\n",
       "      <td>﻿The Project Gutenberg EBook of The King James...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charles Dodgson AKA Lewis Carroll</td>\n",
       "      <td>The Project Gutenberg EBook of Through the Loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lewis Carroll</td>\n",
       "      <td>The Project Gutenberg EBook of The Hunting of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United States. Central Intelligence Agency</td>\n",
       "      <td>The Project Gutenberg EBook of The 1990 CIA Wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       author  \\\n",
       "0                             Abraham Lincoln   \n",
       "1                                     Various   \n",
       "2           Charles Dodgson AKA Lewis Carroll   \n",
       "3                               Lewis Carroll   \n",
       "4  United States. Central Intelligence Agency   \n",
       "\n",
       "                                                text  \n",
       "0  The Project Gutenberg EBook of Lincoln's First...  \n",
       "1  ﻿The Project Gutenberg EBook of The King James...  \n",
       "2  The Project Gutenberg EBook of Through the Loo...  \n",
       "3  The Project Gutenberg EBook of The Hunting of ...  \n",
       "4  The Project Gutenberg EBook of The 1990 CIA Wo...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#update a few of the noticeable errors. Technically, I shouldn't handle it this way since the files may update at any time. Perhaps I'll just leave it as is. :/\n",
    "df['author'][1] = 'Various'\n",
    "df.author[1998] = 'Fannie Isabelle Sherrick'\n",
    "#Clean the text, like for authors.\n",
    "df['text'].apply(lambda x: text_cleaner(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLP generated some kind of an error message when the text exceeded a number of characters. \n",
    "The following steps were taken:\n",
    "> Increased page file size.\n",
    "\n",
    "> Set PC properties to maximize peformance.\n",
    "\n",
    "> Reduced the variable t to translate at that amount. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x='author')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.author == '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have 136 rows of unknown authors. I will assign these to a hold out variable that could, potentially, be used to extract authors using a machine learning model.\n",
    "\n",
    "I shall call the variable - holdout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout = df[df.author == ''].copy()\n",
    "holdout.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.author.replace('', float(\"NaN\"), True)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the authors that are reading as excessively long and update their names so that they register as authors without the extra information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_seq_items = 1000\n",
    "df.author.value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = ['Hesther Lynch Piozzi','Beatrix Potter','Susan Fenimore Cooper','Baker','Hale','Cunninghame Graham','Charles Oliver', 'William Makepeace Thackeray', \n",
    "               \"Shakespeare\", 'James Russell Lowell', 'Henry Wadsworth Longfellow', 'Andrew Dickson White', 'Ayn Rand', 'Fannie Isabelle Sherrick', 'John Lord', 'James Lane Allen',\n",
    "               'Jack London', 'William Wells Brown', 'Thomas A Kempis', \"Alexander Pope\", 'Louis Ginzberg', 'Jerome K. Jerome', 'Maurice Maeterlinck', 'George Wharton James', \n",
    "                'Rudyard Kipling', 'Jerome Lobo', 'James F. Cooper', 'Charles Dickens', 'Upton Sinclair', 'Johanna Spyri', 'William and Ellen Craft', \n",
    "               'James Whitcomb Riley', 'Helen Beecher Long', 'Joseph-Pierre Proudhon', 'Arthur Brisbane', 'Julian Hawthorne', 'R. M. Ballantyne']\n",
    "for corr in corrections:\n",
    "    df.loc[df.author.str.contains(corr), 'author'] = corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of these and more keep turning up. Manual interpretation didn't work out too well for me and neither did my programmatical magic - so I'm going to hold out on these, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copyright = df.loc[df.author.str.contains(\"Copyright\")]\n",
    "holdout = pd.concat([copyright, holdout])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.author.str.contains(\"Copyright\")] = float(\"NaN\")\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the bag of words explanation on this part. I need/want to handle the data cleaning up here so we can see it all at once. \n",
    "\n",
    "There are too many sentences to process the bag of words so I need to reduce my author count. It doesn't make sense to include anonymous/unknown/various authors in my training model. How can I train in that scenario? So - remove multiple authors (authors containing the word and), various, editors, unknown, illustrated, aliases (AKA), anyone author with a parenthsis, and anonymous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unknowns, etc.\n",
    "unknowns = [' and ', 'various', 'anonymous', 'unknown', 'enty years after', 'gutenberg', '\\(', 'edited', 'illustrated']\n",
    "various = df.loc[df.author.str.lower().str.contains('|'.join(unknowns))]\n",
    "\n",
    "holdout = pd.concat([various, holdout])\n",
    "\n",
    "#aliases that are labeled with AKA. \n",
    "alias = df.loc[df.author.str.contains('AKA')]\n",
    "\n",
    "holdout = pd.concat([alias, holdout])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.author.str.lower().str.contains('|'.join(unknowns))] = float(\"NaN\")\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.author.str.contains('AKA')] = float(\"NaN\")\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.author.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balance out the dataset by holding out on all works greater than the mean; that is, reduce those authors count to three. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_counts = df.author.value_counts()\n",
    "keep = author_counts[author_counts.values > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all the works after the first three, because we only want three of them. \n",
    "for a in keep.index:\n",
    "    remove_index = df[df.author.isin([a])][3:].index\n",
    "    remove = df[df.author.isin([a])][3:].copy()\n",
    "    holdout = pd.concat([remove, holdout])\n",
    "    df.drop(remove_index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose 100 authors\n",
    "keep = df.author.value_counts()[0:50]\n",
    "\n",
    "for a in keep.index:\n",
    "    remove_index = df[~df.author.isin([a])].index\n",
    "    remove = df[~df.author.isin([a])].copy()\n",
    "    holdout = pd.concat([remove, holdout])\n",
    "    df.drop(remove_index, inplace=True)\n",
    "holdout.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I should have done something different. This is a fix to get the data back that I accidentally eradicated from the dataframe. \n",
    "df = holdout[holdout.author.isin(keep.index)]\n",
    "holdout = holdout[~holdout.author.isin(keep.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/12/2020 20:13:53 working on doc 0\n",
      "26/12/2020 20:13:54 working on doc 1\n",
      "26/12/2020 20:14:08 working on doc 2\n",
      "26/12/2020 20:14:13 working on doc 3\n",
      "26/12/2020 20:14:14 working on doc 4\n",
      "26/12/2020 20:14:28 working on doc 5\n",
      "26/12/2020 20:14:35 working on doc 6\n",
      "26/12/2020 20:14:48 working on doc 7\n",
      "26/12/2020 20:15:00 working on doc 8\n",
      "26/12/2020 20:15:06 working on doc 9\n",
      "26/12/2020 20:15:19 working on doc 10\n",
      "26/12/2020 20:15:33 working on doc 11\n",
      "26/12/2020 20:15:39 working on doc 12\n",
      "26/12/2020 20:15:48 working on doc 13\n",
      "26/12/2020 20:16:02 working on doc 14\n",
      "26/12/2020 20:16:15 working on doc 15\n",
      "26/12/2020 20:16:28 working on doc 16\n",
      "26/12/2020 20:16:31 working on doc 17\n",
      "26/12/2020 20:16:36 working on doc 18\n",
      "26/12/2020 20:16:47 working on doc 19\n",
      "26/12/2020 20:16:53 working on doc 20\n",
      "26/12/2020 20:17:01 working on doc 21\n",
      "26/12/2020 20:17:14 working on doc 22\n",
      "26/12/2020 20:17:19 working on doc 23\n",
      "26/12/2020 20:17:28 working on doc 24\n",
      "26/12/2020 20:17:33 working on doc 25\n",
      "26/12/2020 20:17:45 working on doc 26\n",
      "26/12/2020 20:17:47 working on doc 27\n",
      "26/12/2020 20:17:49 working on doc 28\n",
      "26/12/2020 20:17:53 working on doc 29\n",
      "26/12/2020 20:17:57 working on doc 30\n",
      "26/12/2020 20:18:09 working on doc 31\n",
      "26/12/2020 20:18:23 working on doc 32\n",
      "26/12/2020 20:18:36 working on doc 33\n",
      "26/12/2020 20:18:49 working on doc 34\n",
      "26/12/2020 20:18:51 working on doc 35\n",
      "26/12/2020 20:19:01 working on doc 36\n",
      "26/12/2020 20:19:14 working on doc 37\n",
      "26/12/2020 20:19:18 working on doc 38\n",
      "26/12/2020 20:19:29 working on doc 39\n",
      "26/12/2020 20:19:36 working on doc 40\n",
      "26/12/2020 20:19:42 working on doc 41\n",
      "26/12/2020 20:19:43 working on doc 42\n",
      "26/12/2020 20:19:44 working on doc 43\n",
      "26/12/2020 20:19:47 working on doc 44\n",
      "26/12/2020 20:19:50 working on doc 45\n",
      "26/12/2020 20:20:00 working on doc 46\n",
      "26/12/2020 20:20:01 working on doc 47\n",
      "26/12/2020 20:20:12 working on doc 48\n",
      "26/12/2020 20:20:28 working on doc 49\n",
      "26/12/2020 20:20:33 working on doc 50\n",
      "26/12/2020 20:20:44 working on doc 51\n",
      "26/12/2020 20:20:53 working on doc 52\n",
      "26/12/2020 20:20:55 working on doc 53\n",
      "26/12/2020 20:20:57 working on doc 54\n",
      "26/12/2020 20:21:04 working on doc 55\n",
      "26/12/2020 20:21:12 working on doc 56\n",
      "26/12/2020 20:21:15 working on doc 57\n",
      "26/12/2020 20:21:28 working on doc 58\n",
      "26/12/2020 20:21:42 working on doc 59\n",
      "26/12/2020 20:21:55 working on doc 60\n",
      "26/12/2020 20:22:08 working on doc 61\n",
      "26/12/2020 20:22:21 working on doc 62\n",
      "26/12/2020 20:22:30 working on doc 63\n",
      "26/12/2020 20:22:44 working on doc 64\n",
      "26/12/2020 20:22:57 working on doc 65\n",
      "26/12/2020 20:23:10 working on doc 66\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:13 working on doc 67\n",
      "26/12/2020 20:23:18 working on doc 68\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:20 working on doc 69\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:21 working on doc 70\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:22 working on doc 71\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:22 working on doc 72\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:22 working on doc 73\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:23 working on doc 74\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:23 working on doc 75\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:23 working on doc 76\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:23 working on doc 77\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:23 working on doc 78\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:24 working on doc 79\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:24 working on doc 80\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:24 working on doc 81\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:24 working on doc 82\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:25 working on doc 83\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:25 working on doc 84\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:25 working on doc 85\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:25 working on doc 86\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:26 working on doc 87\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:26 working on doc 88\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:26 working on doc 89\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:26 working on doc 90\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:26 working on doc 91\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:27 working on doc 92\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:27 working on doc 93\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:27 working on doc 94\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:27 working on doc 95\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:27 working on doc 96\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:28 working on doc 97\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:28 working on doc 98\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:28 working on doc 99\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:28 working on doc 100\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:29 working on doc 101\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:29 working on doc 102\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:29 working on doc 103\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:29 working on doc 104\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:29 working on doc 105\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:30 working on doc 106\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:30 working on doc 107\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:30 working on doc 108\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:30 working on doc 109\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:30 working on doc 110\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:31 working on doc 111\n",
      "encountered an exception, skipping to next document.\n",
      "26/12/2020 20:23:31 working on doc 112\n",
      "26/12/2020 20:23:32 working on doc 113\n",
      "26/12/2020 20:23:42 working on doc 114\n"
     ]
    }
   ],
   "source": [
    "#Now is the time. Turn these text fields into documents.\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = []\n",
    "author = []\n",
    "i = 0\n",
    "df['doc'] = \"\"\n",
    "df['no_stop'] = \"\"\n",
    "# All the processing work takes place below, so it may take a while.\n",
    "for index, t in df.iterrows():\n",
    "    print(f'{dt.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")} working on doc {i}')\n",
    "    #try to process the text file into a document. Skip it if it can't. As many as possible.\n",
    "    try:\n",
    "        #500000 characters seems long enough for this exercise. I just want to identify the author - which is a text field labled \"Author:\" or whoever is after the \"by\" statement. \n",
    "        t['doc'] = nlp(t.text[0:500000])\n",
    "        t['no_stop'] =  [token for token in t.doc if not token.is_stop]\n",
    "        i += 1\n",
    "    except:\n",
    "        i += 1\n",
    "        print(\"encountered an exception, skipping to next document.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = [len(w) for w in df[df.author == 'Honore de Balzac']['doc'].iloc[0]]\n",
    "fdist = nltk.FreqDist(length)\n",
    "fdist.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate your list of the most common words\n",
    "word_freq = word_frequencies(df[df.author == 'Honore de Balzac']['doc'].iloc[0]).most_common(10)\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by sentences for each of these documents. \n",
    "list_sentences = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    list_sentences += ([[sent, row.author, row.doc[0:500]] for sent in row.doc.sents])\n",
    "    \n",
    "# Combine the sentences from the two novels into one DataFrame\n",
    "sentences = pd.DataFrame(list_sentences, columns = [\"sentences\", \"author\", 'title'])\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of stop words and punctuation and lemmatize the tokens\n",
    "for i, sentence in enumerate(sentences[\"sentences\"]):\n",
    "    sentences.loc[i, \"sentences\"] = [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing is taking over 4 hours to complete. It takes about a half hour to complete download the files from the internet. \n",
    "\n",
    "I am exporting the data object using pickle so that I can access the object in case I have to reboot. \n",
    "I've got to run a grid search, yet. \n",
    "\n",
    "I receive the following error when I attempt to pickle the tokens and sentences:\n",
    "```\n",
    "NotImplementedError: [E111] Pickling a token is not supported, because tokens are only views of the parent Doc and can't exist on their own. A pickled token would always have to include its Doc and Vocab, which has practically no advantage over pickling the parent Doc directly. So instead of pickling the token, pickle the Doc it belongs to.\n",
    "```\n",
    "\n",
    "The pickle file, at this juncture (100 authors) , is well over 25 GB, so I'm saving just the doc and the author from the dataframe. \n",
    "\n",
    "Last, but not least, the file is being saved outside of the project folder because it's too big for GitHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickle the author and doc in case I have to reboot. \n",
    "timer('pickling dataframes.')\n",
    "\n",
    "with open(\"..\\d_store\\data.pkl\", 'wb') as f:\n",
    "    pickle.dump(df[['author', 'doc']], f)\n",
    "\n",
    "timer('done pickling dataframes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well when I was processing at 100 authors, I could write the file, but I not read it. \n",
    "\n",
    "I got the following \n",
    "``` MemoryError: Error assigning 4916400 bytes ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer(\"opening the data\")\n",
    "with open(\"..\\d_store\\data.pkl\", 'rb') as f:\n",
    "    blap = pickle.load(f)\n",
    "timer(\"done opening the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(blap.doc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not give joblib a shot? It could work. I hear I can use pipelines with it. Let's see if I can get this one to cut the mustard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer(\"saving\")\n",
    "joblib.dump(df, \"..\\d_store\\data.sav\")\n",
    "timer('done saving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/12/2020 19:11:19: loading\n"
     ]
    }
   ],
   "source": [
    "timer('loading')\n",
    "blaj = joblib.load(\"..\\d_store\\data.sav\")\n",
    "timer('done loading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(blaj.doc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bag of words, I have to troubleshoot a memory error. \n",
    "\n",
    "`MemoryError: Unable to allocate 27.0 TiB for an array with shape (6566859, 565300) and data type int64`\n",
    "\n",
    "Step One: Reduce the size by changing the data type.\n",
    "\n",
    "> Another error arises when I attempt to mask the shape to an int8.\n",
    "\n",
    ">```\n",
    "import numpy as np\n",
    "mask = np.zeros(X.shape,dtype='uint8')\n",
    ">```\n",
    "    \n",
    ">`MemoryError: Unable to allocate 3.38 TiB for an array with shape (6566859, 565300) and data type uint8`\n",
    "\n",
    "\n",
    "Step Two: Reduce content by reducing authors.\n",
    "> Project requirements said ten authors and thousands of texts, with at least seven novels. \n",
    "\n",
    "> I returned to the data cleaning process to eliminate various/anonymous/unknown authors.\n",
    "\n",
    "> I still get an error.\n",
    "\n",
    "> `MemoryError: Unable to allocate 16.1 TiB for an array with shape (5928859, 373831) and data type int64`\n",
    "\n",
    "Step Three: Reduce content again by reducing authors in a way such that the dataset is more balanced. \n",
    ">I returned to the data cleaning process to reduce the number of works of those largest authors to three or four. \n",
    "\n",
    ">Received a similar memory error. \n",
    "\n",
    "Step Four: Skip the dataframe, train the model.\n",
    "\n",
    "> Received a similar memory error and the entire system locked up. \n",
    "\n",
    "Step Five: Reduce the number of authors.  \n",
    "> ```Unable to allocate 7.31 GiB for an array with shape (2952352, 665) and data type int32```\n",
    "\n",
    "> Return to data cleaning and choose the top 100 authors. Decrease steadily until the program runs. \n",
    "\n",
    "In the interest of keeping things easy and to the point; I'll be refraining from adding the error messages. The logistic regression was taking an excessive amount of time to run. I attempted to shape the data into a dataframe again and received an error mentioning 2.77 TiB for 100 authors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sentences['author']\n",
    "X = sentences['sentences']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "X_train = vectorizer.fit_transform(X_train.astype(str))\n",
    "X_test = vectorizer.transform(X_test.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.77 TiB for an array with shape (2314536, 164250) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-3a72d573fc7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbow_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbow_sents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbow_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"author\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1183\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.77 TiB for an array with shape (2314536, 164250) and data type int64"
     ]
    }
   ],
   "source": [
    "bow_df = pd.DataFrame(X_train.toarray(), columns=vectorizer.get_feature_names())\n",
    "bow_sents = pd.concat([bow_df, sentences[[\"text\", \"author\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-3da6089501ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Test on logistic regression to see if it runs. Expand from here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1415\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1416\u001b[0m                       sample_weight=sample_weight)\n\u001b[1;32m-> 1417\u001b[1;33m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[0;32m   1418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[0;32m    758\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"L-BFGS-B\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 760\u001b[1;33m                 \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"iprint\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0miprint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"gtol\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"maxiter\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m             )\n\u001b[0;32m    762\u001b[0m             n_iter_i = _check_optimize_result(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m--> 618\u001b[1;33m                                 callback=callback, **options)\n\u001b[0m\u001b[0;32m    619\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[1;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;34m\"\"\" returns the the function value \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfunc\u001b[1;34m(x, *args)\u001b[0m\n\u001b[0;32m    733\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_multi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 735\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0m_multinomial_loss_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    736\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'newton-cg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0m_multinomial_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_multinomial_loss_grad\u001b[1;34m(w, X, Y, alpha, sample_weight)\u001b[0m\n\u001b[0;32m    345\u001b[0m     grad = np.zeros((n_classes, n_features + bool(fit_intercept)),\n\u001b[0;32m    346\u001b[0m                     dtype=X.dtype)\n\u001b[1;32m--> 347\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_multinomial_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m     \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_multinomial_loss\u001b[1;34m(w, X, Y, alpha, sample_weight)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mintercept\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msquared_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Test on logistic regression to see if it runs. Expand from here. \n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(classification_report(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = model_results('bow', X_train.toarray(), X_test.toarray(), y_train, y_test)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some constant variables for parameter settings. \n",
    "#cross validation, constant parameter\n",
    "cv = 5\n",
    "\n",
    "#multiclass\n",
    "mc = ['auto', 'ovr', 'multinomial']\n",
    "#solver\n",
    "sv = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "#penalty\n",
    "py = ['l1', 'l2', 'elasticnet', 'none']\n",
    "#criterion\n",
    "cn = ['gini', 'entropy']\n",
    "#max_features\n",
    "mf = ['auto', 'sqrt', 'log2']\n",
    "#splitter\n",
    "sr = ['best', 'random']\n",
    "#max_depth\n",
    "md = [5, 7, 15, 29] \n",
    "#n_estimators\n",
    "ne = [n+50 for n in range(50, 500, 50)]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression\n",
    "lrp = {    \"lr__multi_class\": mc, \n",
    "           \"lr__solver\": sv,\n",
    "           'lr__penalty' : py,\n",
    "      }\n",
    "#decision tree\n",
    "dtp = {\n",
    "           'dt__criterion': cn,\n",
    "           'dt__max_features': mf,\n",
    "           'dt__splitter':sr, \n",
    "           'dt__max_depth': md     \n",
    "}\n",
    "#K Nearest Neighbors\n",
    "knnp = {\n",
    "           'knn__n_neighbors': [3, 9, 39, 12],\n",
    "           'knn__weights': ['uniform', 'distance'],\n",
    "           'knn__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], \n",
    "           'knn__leaf_size': [30, 12, 9, 3, 17],  \n",
    "           'knn__metric': ['mahalanobis','euclidean', 'manhattan', 'chebyshev', 'minkowski', 'seuclidean'],  \n",
    "}\n",
    "#Support Vector Classifier\n",
    "svp = {\n",
    "          'sv__kernel': ['linear', 'rbf'],\n",
    "          'sv__break_ties': [True, False],\n",
    "}\n",
    "#Random Forest Classifier \n",
    "rfp = {\n",
    "           'rf__criterion': cn,\n",
    "           'rf__max_features': mf,\n",
    "           'rf__oob_score':[True, False], \n",
    "           'rf__max_depth': md,\n",
    "           'rf__n_estimators': ne,     \n",
    "}\n",
    "#gradient boost \n",
    "gbp = {    'gb__criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "           'gb__max_features': mf,\n",
    "           'gb__max_depth': md, \n",
    "           'gb__n_estimators': ne\n",
    "          }\n",
    "transform = { 'vectorizer__analyzer': word }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Update pipeline with parameters and classifiers.\n",
    "pipeline = Pipeline([('vectorizer', CountVectorizer(analyzer='word')), ('nb', ComplementNB())])\n",
    "params = transform\n",
    "search = GridSearchCV(pipeline, params, cv=cv)\n",
    "print(f'{dt.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}, started grid search')\n",
    "search.fit(X_train, y_train)\n",
    "print(f'{dt.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}, grid search complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ",\n",
    "                     ('rf', RandomForestClassifier(random_state=1)),\n",
    "                     ('lr', LogisticRegression()),\n",
    "                     ('gb', GradientBoostingClassifier()),\n",
    "                     ('knn', KNeighborsClassifier()),\n",
    "                     ('sv', SVC()),\n",
    "                     ('dt', DecisionTreeClassifier())\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_model = search.best_estimator_\n",
    "\n",
    "bow_model.fit(X_train, y_train)\n",
    "y_pred = bow_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
